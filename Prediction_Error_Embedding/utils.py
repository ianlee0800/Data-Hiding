import numpy as np
import pandas as pd
import cv2
import matplotlib.pyplot as plt
import os
import time
import traceback
from scipy.signal import savgol_filter
from tqdm import tqdm
from datetime import datetime
import cupy as cp
from prettytable import PrettyTable

from common import (
    calculate_psnr,
    calculate_ssim,
    histogram_correlation,
    cleanup_memory
)
from image_processing import (
    save_image,
    generate_histogram,
    PredictionMethod
)



# =============================================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºæœ¬å·¥å…·å‡½æ•¸
# =============================================================================

def generate_random_binary_array(size, ratio_of_ones=0.5):
    """ç”ŸæˆæŒ‡å®šå¤§å°çš„éšæœºäºŒè¿›åˆ¶æ•°ç»„ï¼Œå¯è°ƒæ•´1çš„æ¯”ä¾‹"""
    return np.random.choice([0, 1], size=size, p=[1-ratio_of_ones, ratio_of_ones])

def ensure_dir(file_path):
    """ç¢ºä¿ç›®éŒ„å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡å‰µå»º"""
    directory = os.path.dirname(file_path)
    if not os.path.exists(directory):
        os.makedirs(directory)

# =============================================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šPEE è³‡è¨Šè¡¨æ ¼ç›¸é—œåŠŸèƒ½
# =============================================================================

def create_pee_info_table(pee_stages, use_different_weights, total_pixels, 
                         split_size, quad_tree=False):
    """
    å‰µå»º PEE è³‡è¨Šè¡¨æ ¼çš„å®Œæ•´å‡½æ•¸ - æ”¯æ´å½©è‰²åœ–åƒ
    """    
    table = PrettyTable()
    
    # ğŸ”§ ä¿®æ”¹ï¼šæ”¹é€²å½©è‰²åœ–åƒæª¢æ¸¬é‚è¼¯
    is_color_image = False
    if pee_stages and 'channel_payloads' in pee_stages[0]:
        # æ–°çš„æª¢æ¸¬æ–¹å¼ï¼šæª¢æŸ¥æ˜¯å¦æœ‰channel_payloadsæ¬„ä½
        is_color_image = True
    elif pee_stages and 'block_info' in pee_stages[0]:
        # èˆŠçš„æª¢æ¸¬æ–¹å¼ï¼šæª¢æŸ¥block_infoä¸­æ˜¯å¦æœ‰é€šé“æ¨™è­˜
        if isinstance(pee_stages[0]['block_info'], dict):
            for size_str, size_info in pee_stages[0]['block_info'].items():
                if isinstance(size_info, dict) and 'blocks' in size_info:
                    for block in size_info['blocks']:
                        if 'channel' in block:
                            is_color_image = True
                            break
                    if is_color_image:
                        break
    
    if is_color_image:
        # å½©è‰²åœ–åƒçš„è¡¨æ ¼æ¬„ä½
        table.field_names = [
            "Embedding", "Total Payload", "BPP", "PSNR", "SSIM", "Hist Corr",
            "Blue Payload", "Green Payload", "Red Payload", "Block Counts", "Note"
        ]
        
        # ğŸ”§ ä¿®æ”¹ï¼šç°¡åŒ–å½©è‰²åœ–åƒçš„è¡¨æ ¼å…§å®¹ï¼Œé‡é»é¡¯ç¤ºç¸½é«”ä¿¡æ¯
        for stage in pee_stages:
            # æ·»åŠ æ•´é«” stage è³‡è¨Š
            table.add_row([
                stage['embedding'],
                stage['payload'],
                f"{stage['bpp']:.4f}",
                f"{stage['psnr']:.2f}",
                f"{stage['ssim']:.4f}",
                f"{stage['hist_corr']:.4f}",
                stage['channel_payloads']['blue'],
                stage['channel_payloads']['green'], 
                stage['channel_payloads']['red'],
                sum(stage.get('block_counts', {}).values()) if 'block_counts' in stage else '-',
                "Color Image"
            ])
            
            # æ·»åŠ åˆ†éš”ç·š
            table.add_row(["-" * 5] * len(table.field_names))
    
    elif quad_tree:
        # Quad tree æ¨¡å¼çš„è¡¨æ ¼æ¬„ä½ï¼ˆä¿æŒä¸è®Šï¼‰
        table.field_names = [
            "Embedding", "Block Size", "Block Position", "Payload", "BPP",
            "PSNR", "SSIM", "Hist Corr", "Weights", "EL", "Note"
        ]
        
        # è™•ç† quad tree æ¨¡å¼çš„å€å¡Šè³‡è¨Šï¼ˆä¿æŒåŸé‚è¼¯ï¼‰
        for stage in pee_stages:
            # æ·»åŠ æ•´é«” stage è³‡è¨Š
            table.add_row([
                f"{stage['embedding']} (Overall)",
                "-",
                "-", 
                stage['payload'],
                f"{stage['bpp']:.4f}",
                f"{stage['psnr']:.2f}",
                f"{stage['ssim']:.4f}",
                f"{stage['hist_corr']:.4f}",
                "-",
                "-",
                "Stage Summary"
            ])
            
            # æ·»åŠ åˆ†éš”ç·š
            table.add_row(["-" * 5] * len(table.field_names))
            
            # è™•ç† quad tree æ¨¡å¼çš„å€å¡Šè³‡è¨Š
            for size_str in sorted(stage['block_info'].keys(), key=int, reverse=True):
                blocks = stage['block_info'][size_str]['blocks']
                for block in blocks:
                    block_pixels = block['size'] * block['size']
                    
                    # è™•ç†æ¬Šé‡é¡¯ç¤º
                    weights_display = (
                        "N/A" if block['weights'] == "N/A"
                        else ", ".join([f"{w:.2f}" for w in block['weights']]) if block['weights']
                        else "-"
                    )
                    
                    table.add_row([
                        stage['embedding'],
                        f"{block['size']}x{block['size']}",
                        f"({block['position'][0]}, {block['position'][1]})",
                        block['payload'],
                        f"{block['payload'] / block_pixels:.4f}",
                        f"{block['psnr']:.2f}",
                        f"{block['ssim']:.4f}",
                        f"{block['hist_corr']:.4f}",
                        weights_display,
                        block.get('EL', '-'),
                        "Different weights" if use_different_weights else ""
                    ])
            
            # æ·»åŠ åˆ†éš”ç·š
            table.add_row(["-" * 5] * len(table.field_names))
            
    else:
        # æ¨™æº–æ¨¡å¼çš„è¡¨æ ¼æ¬„ä½ï¼ˆä¿æŒä¸è®Šï¼‰
        table.field_names = [
            "Embedding", "Sub-image", "Payload", "BPP", "PSNR", "SSIM",
            "Hist Corr", "Weights", "EL", "Rotation", "Note"
        ]
        
        # è™•ç†æ¨™æº–æ¨¡å¼ï¼ˆä¿æŒåŸé‚è¼¯ï¼‰
        for stage in pee_stages:
            # æ·»åŠ æ•´é«” stage è³‡è¨Š
            table.add_row([
                f"{stage['embedding']} (Overall)",
                "-",
                stage['payload'],
                f"{stage['bpp']:.4f}",
                f"{stage['psnr']:.2f}",
                f"{stage['ssim']:.4f}",
                f"{stage['hist_corr']:.4f}",
                "-",
                "-",
                "-",
                "Stage Summary"
            ])
            
            # æ·»åŠ åˆ†éš”ç·š
            table.add_row(["-" * 5] * len(table.field_names))
            
            # è™•ç†æ¨™æº–æ¨¡å¼çš„å€å¡Šè³‡è¨Š
            total_blocks = split_size * split_size
            sub_image_pixels = total_pixels // total_blocks
            
            for i, block in enumerate(stage['block_params']):
                # Process weights display with better error handling
                weights_display = "-"
                if 'weights' in block:
                    if block['weights'] == "N/A":
                        weights_display = "N/A"
                    elif block['weights']:
                        try:
                            weights_display = ", ".join([f"{w:.2f}" for w in block['weights']])
                        except:
                            weights_display = str(block['weights'])
                
                # Get other fields with defaults if missing
                payload = block.get('payload', 0)
                psnr = block.get('psnr', 0)
                ssim = block.get('ssim', 0)
                hist_corr = block.get('hist_corr', 0)
                el = block.get('EL', '-')
                rotation = block.get('rotation', 0)
                
                table.add_row([
                    stage['embedding'] if i == 0 else "",
                    i,
                    payload,
                    f"{payload / sub_image_pixels:.4f}",
                    f"{psnr:.2f}",
                    f"{ssim:.4f}",
                    f"{hist_corr:.4f}",
                    weights_display,
                    el,
                    f"{rotation}Â°",
                    "Different weights" if use_different_weights else ""
                ])
        
        # æ·»åŠ åˆ†éš”ç·š
        table.add_row(["-" * 5] * len(table.field_names))
    
    return table

# =============================================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šåµŒå…¥æ•¸æ“šç”Ÿæˆç›¸é—œå‡½æ•¸
# =============================================================================

def generate_embedding_data(total_embeddings, sub_images_per_stage, max_capacity_per_subimage, 
                           ratio_of_ones=0.5, target_payload_size=-1):
    """
    ç”ŸæˆåµŒå…¥æ•¸æ“š
    
    Parameters:
    -----------
    total_embeddings : int
        ç¸½åµŒå…¥éšæ®µæ•¸
    sub_images_per_stage : int
        æ¯å€‹stageçš„å­åœ–åƒæ•¸é‡
    max_capacity_per_subimage : int
        æ¯å€‹å­åœ–åƒçš„æœ€å¤§å®¹é‡
    ratio_of_ones : float, optional
        ç”Ÿæˆæ•¸æ“šä¸­1çš„æ¯”ä¾‹ï¼Œé»˜èªç‚º0.5
    target_payload_size : int, optional
        ç›®æ¨™ç¸½payloadå¤§å°ï¼Œè¨­ç‚º-1æˆ–0æ™‚ä½¿ç”¨æœ€å¤§å®¹é‡
        
    Returns:
    --------
    dict
        åŒ…å«æ¯å€‹stageçš„æ•¸æ“šç”Ÿæˆè³‡è¨Š
    """
    # å¦‚æœæ²’æœ‰æŒ‡å®šç›®æ¨™payloadï¼Œä½¿ç”¨æœ€å¤§å®¹é‡æ¨¡å¼
    if target_payload_size <= 0:
        max_stage_payload = sub_images_per_stage * max_capacity_per_subimage
        stage_data = []
        for _ in range(total_embeddings):
            sub_data_list = []
            for _ in range(sub_images_per_stage):
                sub_data = generate_random_binary_array(max_capacity_per_subimage, ratio_of_ones)
                sub_data_list.append(sub_data)
            stage_data.append({
                'sub_data': sub_data_list,
                'remaining_target': 0
            })
        return {
            'stage_data': stage_data,
            'total_target': max_stage_payload * total_embeddings
        }
    
    # ä½¿ç”¨æŒ‡å®šçš„payload size
    total_remaining = target_payload_size
    stage_data = []
    
    # ç‚ºæ¯å€‹stageåˆ†é…æ½›åœ¨çš„æœ€å¤§å®¹é‡
    potential_capacity_per_stage = max_capacity_per_subimage * sub_images_per_stage
    
    for stage in range(total_embeddings):
        sub_data_list = []
        
        for sub_img in range(sub_images_per_stage):
            # è¨ˆç®—é€™å€‹å­åœ–åƒå¯èƒ½éœ€è¦çš„æœ€å¤§æ•¸æ“šé‡
            max_possible = min(max_capacity_per_subimage, total_remaining)
            
            # å¦‚æœæ˜¯æœ€å¾Œä¸€å€‹stageçš„æœ€å¾Œä¸€å€‹å­åœ–åƒï¼Œç¢ºä¿ç”Ÿæˆè¶³å¤ çš„æ•¸æ“š
            if stage == total_embeddings - 1 and sub_img == sub_images_per_stage - 1:
                sub_data = generate_random_binary_array(total_remaining, ratio_of_ones)
            else:
                sub_data = generate_random_binary_array(max_possible, ratio_of_ones)
            
            sub_data_list.append(sub_data)
        
        stage_data.append({
            'sub_data': sub_data_list,
            'remaining_target': total_remaining  # è¨˜éŒ„ç•¶å‰éšæ®µé‚„éœ€è¦åµŒå…¥å¤šå°‘æ•¸æ“š
        })
    
    return {
        'stage_data': stage_data,
        'total_target': target_payload_size
    }

# =============================================================================
# ç¬¬å››éƒ¨åˆ†ï¼šç²¾ç¢ºæ¸¬é‡ç›¸é—œå‡½æ•¸
# =============================================================================

def run_embedding_with_target(origImg, method, prediction_method, ratio_of_ones, 
                             total_embeddings, el_mode, target_payload_size,
                             split_size=2, block_base=False, quad_tree_params=None,
                             use_different_weights=False):
    """
    åŸ·è¡Œç‰¹å®šåµŒå…¥ç®—æ³•ï¼Œé‡å°ç‰¹å®šçš„ç›®æ¨™payload
    
    Parameters:
    -----------
    origImg : numpy.ndarray
        åŸå§‹åœ–åƒ
    method : str
        ä½¿ç”¨çš„æ–¹æ³• ("rotation", "split", "quadtree")
    prediction_method : PredictionMethod
        é æ¸¬æ–¹æ³•
    ratio_of_ones : float
        åµŒå…¥æ•¸æ“šä¸­1çš„æ¯”ä¾‹
    total_embeddings : int
        ç¸½åµŒå…¥æ¬¡æ•¸
    el_mode : int
        ELæ¨¡å¼
    target_payload_size : int
        ç›®æ¨™åµŒå…¥é‡
    split_size : int, optional
        åˆ†å‰²å¤§å°
    block_base : bool, optional
        æ˜¯å¦ä½¿ç”¨block baseæ–¹å¼
    quad_tree_params : dict, optional
        å››å‰æ¨¹åƒæ•¸
    use_different_weights : bool, optional
        æ˜¯å¦ä½¿ç”¨ä¸åŒæ¬Šé‡
        
    Returns:
    --------
    tuple
        (final_img, actual_payload, stages)
    """
    from embedding import (
        pee_process_with_rotation_cuda,
        pee_process_with_split_cuda
    )
    from quadtree import pee_process_with_quadtree_cuda
    # é‡ç½®GPUè¨˜æ†¶é«”
    cp.get_default_memory_pool().free_all_blocks()
    
    # æ ¹æ“šæ–¹æ³•é¸æ“‡ç›¸æ‡‰çš„åµŒå…¥ç®—æ³•
    if method == "rotation":
        final_img, actual_payload, stages = pee_process_with_rotation_cuda(
            origImg,
            total_embeddings,
            ratio_of_ones,
            use_different_weights,
            split_size,
            el_mode,
            prediction_method=prediction_method,
            target_payload_size=target_payload_size
        )
    elif method == "split":
        final_img, actual_payload, stages = pee_process_with_split_cuda(
            origImg,
            total_embeddings,
            ratio_of_ones,
            use_different_weights,
            split_size,
            el_mode,
            block_base,
            prediction_method=prediction_method,
            target_payload_size=target_payload_size
        )
    elif method == "quadtree":
        if quad_tree_params is None:
            quad_tree_params = {
                'min_block_size': 16,
                'variance_threshold': 300
            }
        
        final_img, actual_payload, stages = pee_process_with_quadtree_cuda(
            origImg,
            total_embeddings,
            ratio_of_ones,
            use_different_weights,
            quad_tree_params['min_block_size'],
            quad_tree_params['variance_threshold'],
            el_mode,
            rotation_mode='random',
            prediction_method=prediction_method,
            target_payload_size=target_payload_size
        )
    else:
        raise ValueError(f"Unknown method: {method}")
    
    return final_img, actual_payload, stages

def ensure_bpp_psnr_consistency(results_df):
    """
    ç¢ºä¿ BPP-PSNR æ•¸æ“šçš„ä¸€è‡´æ€§ï¼šè¼ƒé«˜çš„ BPP æ‡‰æœ‰è¼ƒä½çš„ PSNR
    """
    df = results_df.copy().sort_values('BPP')
    
    # ç¢ºä¿ PSNR éš¨è‘— BPP å¢åŠ è€Œå–®èª¿ä¸‹é™
    for i in range(1, len(df)):
        if df.iloc[i]['PSNR'] > df.iloc[i-1]['PSNR']:
            # ç•°å¸¸é»æª¢æ¸¬ï¼šç•¶å‰ PSNR é«˜æ–¼å‰ä¸€å€‹é»
            if i > 1:
                # ä½¿ç”¨å‰å…©å€‹é»çš„å¹³å‡æ–œç‡é€²è¡Œä¿®æ­£
                prev_slope = (df.iloc[i-1]['PSNR'] - df.iloc[i-2]['PSNR']) / (df.iloc[i-1]['BPP'] - df.iloc[i-2]['BPP'])
                expected_drop = prev_slope * (df.iloc[i]['BPP'] - df.iloc[i-1]['BPP'])
                corrected_psnr = max(df.iloc[i-1]['PSNR'] + expected_drop, df.iloc[i-1]['PSNR'] * 0.995)
                df.loc[df.index[i], 'PSNR'] = corrected_psnr
            else:
                # ç°¡å–®åœ°å°‡ PSNR è¨­ç‚ºç¨ä½æ–¼å‰ä¸€å€‹é»
                df.loc[df.index[i], 'PSNR'] = df.iloc[i-1]['PSNR'] * 0.995
    
    # å° SSIM å’Œ Hist_Corr ä¹Ÿé€²è¡Œé¡ä¼¼çš„è™•ç†
    for metric in ['SSIM', 'Hist_Corr']:
        for i in range(1, len(df)):
            if df.iloc[i][metric] > df.iloc[i-1][metric]:
                # ç°¡å–®åœ°é€²è¡Œå¹³æ»‘è™•ç†
                df.loc[df.index[i], metric] = df.iloc[i-1][metric] * 0.99
    
    return df.sort_values('Target_Percentage')

def run_precise_measurements(origImg, imgName, method, prediction_method, ratio_of_ones, 
                            total_embeddings=5, el_mode=0, segments=15, step_size=None, use_different_weights=False,
                            split_size=2, block_base=False, quad_tree_params=None):
    """
    é‹è¡Œç²¾ç¢ºçš„æ•¸æ“šé»æ¸¬é‡ï¼Œç‚ºå‡å‹»åˆ†å¸ƒçš„payloadç›®æ¨™å–®ç¨åŸ·è¡ŒåµŒå…¥ç®—æ³•
    å¢åŠ æ•¸æ“šå¹³æ»‘è™•ç†å’Œç•°å¸¸é»ä¿®æ­£åŠŸèƒ½ï¼Œä¸¦ç¢ºä¿æœ€å¤§å®¹é‡é»ä½¿ç”¨åˆå§‹æ¸¬é‡å€¼
    
    Parameters:
    -----------
    origImg : numpy.ndarray
        åŸå§‹åœ–åƒ
    imgName : str
        åœ–åƒåç¨± (ç”¨æ–¼ä¿å­˜çµæœ)
    method : str
        ä½¿ç”¨çš„æ–¹æ³•
    prediction_method : PredictionMethod
        é æ¸¬æ–¹æ³•
    ratio_of_ones : float
        åµŒå…¥æ•¸æ“šä¸­1çš„æ¯”ä¾‹
    total_embeddings : int
        ç¸½åµŒå…¥æ¬¡æ•¸
    el_mode : int
        ELæ¨¡å¼
    segments : int
        è¦æ¸¬é‡çš„æ•¸æ“šé»æ•¸é‡ (å¦‚æœæä¾›äº†step_sizeå‰‡å¿½ç•¥æ­¤åƒæ•¸)
    step_size : int, optional
        æ¸¬é‡é»ä¹‹é–“çš„æ­¥é•· (ä»¥ä½å…ƒç‚ºå–®ä½ï¼Œä¾‹å¦‚10000)
        å¦‚æœæä¾›ï¼Œå‰‡è¦†è“‹segmentsåƒæ•¸
    use_different_weights : bool
        æ˜¯å¦ä½¿ç”¨ä¸åŒæ¬Šé‡
    split_size : int
        åˆ†å‰²å¤§å°
    block_base : bool
        æ˜¯å¦ä½¿ç”¨block baseæ–¹å¼
    quad_tree_params : dict
        å››å‰æ¨¹åƒæ•¸
        
    Returns:
    --------
    pandas.DataFrame
        åŒ…å«æ‰€æœ‰æ¸¬é‡çµæœçš„DataFrame
    """
    # ç¸½é‹è¡Œé–‹å§‹æ™‚é–“
    total_start_time = time.time()
    
    # å‰µå»ºçµæœç›®éŒ„
    if isinstance(prediction_method, str):
        method_name = prediction_method
    else:
        method_name = prediction_method.value
        
    result_dir = f"./Prediction_Error_Embedding/outcome/plots/{imgName}/precise_{method_name}"
    os.makedirs(result_dir, exist_ok=True)
    
    # è¨˜éŒ„é‹è¡Œè¨­ç½®
    log_file = f"{result_dir}/precise_measurements_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    with open(log_file, 'w') as f:
        f.write(f"Precise measurement run started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Image: {imgName}\n")
        f.write(f"Method: {method}\n")
        f.write(f"Prediction method: {method_name}\n")
        f.write(f"Ratio of ones: {ratio_of_ones}\n")
        f.write(f"Total embeddings: {total_embeddings}\n")
        f.write(f"EL mode: {el_mode}\n")
        if step_size:
            f.write(f"Step size: {step_size} bits\n")
        else:
            f.write(f"Segments: {segments}\n")
        f.write(f"Use different weights: {use_different_weights}\n")
        f.write("\n" + "="*80 + "\n\n")
    
    # æ­¥é©Ÿ1: æ‰¾å‡ºæœ€å¤§åµŒå…¥å®¹é‡
    print(f"\n{'='*80}")
    print(f"Step 1: Finding maximum payload capacity")
    print(f"{'='*80}")
    
    with open(log_file, 'a') as f:
        f.write(f"Step 1: Finding maximum payload capacity\n")
    
    start_time = time.time()
    final_img_max, max_payload, stages_max = run_embedding_with_target(
        origImg, method, prediction_method, ratio_of_ones, 
        total_embeddings, el_mode, target_payload_size=-1,
        split_size=split_size, block_base=block_base, 
        quad_tree_params=quad_tree_params,
        use_different_weights=use_different_weights
    )
    
    max_run_time = time.time() - start_time
    total_pixels = origImg.size
    
    # è¨ˆç®—æœ€å¤§å®¹é‡çš„å“è³ªæŒ‡æ¨™
    psnr_max = calculate_psnr(origImg, final_img_max)
    ssim_max = calculate_ssim(origImg, final_img_max)
    hist_corr_max = histogram_correlation(
        np.histogram(origImg, bins=256, range=(0, 255))[0],
        np.histogram(final_img_max, bins=256, range=(0, 255))[0]
    )
    
    # å‰µå»ºæœ€å¤§å®¹é‡çµæœå­—å…¸
    max_capacity_result = {
        'Target_Percentage': 100.0,
        'Target_Payload': max_payload,
        'Actual_Payload': max_payload,
        'BPP': max_payload / total_pixels,
        'PSNR': psnr_max,
        'SSIM': ssim_max,
        'Hist_Corr': hist_corr_max,
        'Processing_Time': max_run_time,
        'Suspicious': False  # åˆå§‹æ¸¬é‡ä¸æ¨™è¨˜ç‚ºå¯ç–‘
    }
    
    print(f"Maximum payload: {max_payload} bits")
    print(f"Max BPP: {max_payload/total_pixels:.6f}")
    print(f"Max PSNR: {psnr_max:.2f}")
    print(f"Max SSIM: {ssim_max:.4f}")
    print(f"Time taken: {max_run_time:.2f} seconds")
    
    with open(log_file, 'a') as f:
        f.write(f"Maximum payload: {max_payload} bits\n")
        f.write(f"Max BPP: {max_payload/total_pixels:.6f}\n")
        f.write(f"Max PSNR: {psnr_max:.2f}\n")
        f.write(f"Max SSIM: {ssim_max:.4f}\n")
        f.write(f"Max Hist Corr: {hist_corr_max:.4f}\n")
        f.write(f"Time taken: {max_run_time:.2f} seconds\n\n")
    
    # ä¿å­˜æœ€å¤§å®¹é‡çš„åµŒå…¥åœ–åƒ
    save_image(final_img_max, f"{result_dir}/embedded_100pct.png")
    
    # æ­¥é©Ÿ2: è¨ˆç®—æ¸¬é‡é»ï¼Œæ’é™¤æœ€å¤§å®¹é‡é»ï¼ˆå› ç‚ºå·²ç¶“æ¸¬é‡ï¼‰
    print(f"\n{'='*80}")
    if step_size and step_size > 0:
        print(f"Step 2: Calculating measurement points with {step_size} bit steps")
    else:
        print(f"Step 2: Calculating {segments} evenly distributed payload points")
    print(f"{'='*80}")
    
    with open(log_file, 'a') as f:
        if step_size and step_size > 0:
            f.write(f"Step 2: Calculating measurement points with {step_size} bit steps\n")
        else:
            f.write(f"Step 2: Calculating {segments} evenly distributed payload points\n")
    
    # æ ¹æ“šåƒæ•¸ç”Ÿæˆæ¸¬é‡é»ï¼Œä½†æ’é™¤æœ€å¤§å®¹é‡é»
    if step_size and step_size > 0:
        # ä½¿ç”¨å›ºå®šæ­¥é•·ç”Ÿæˆæ¸¬é‡é»
        payload_points = list(range(step_size, max_payload, step_size))
    else:
        # ä½¿ç”¨åˆ†æ®µç”Ÿæˆæ¸¬é‡é»ï¼Œä½†æ’é™¤100%é»
        payload_points = [int(max_payload * (i+1) / segments) for i in range(segments-1)]
        # æ·»åŠ æœ€å¾Œä¸€å€‹é»ä½œç‚ºæ¥è¿‘æœ€å¤§å€¼çš„é»ï¼ˆä¾‹å¦‚99%ï¼‰ï¼Œå¦‚æœéœ€è¦çš„è©±
        if segments > 1:
            payload_points.append(int(max_payload * 0.99))
    
    # ç¢ºä¿æ¸¬é‡é»ä¸­ä¸åŒ…å«æœ€å¤§å®¹é‡
    if max_payload in payload_points:
        payload_points.remove(max_payload)
    
    print("Target payload points:")
    for i, target in enumerate(payload_points):
        print(f"  Point {i+1}: {target} bits ({target/max_payload*100:.1f}% of max)")
    print(f"  Point {len(payload_points)+1}: {max_payload} bits (100.0% of max) [using initial measurement]")
    
    with open(log_file, 'a') as f:
        f.write("Target payload points:\n")
        for i, target in enumerate(payload_points):
            f.write(f"  Point {i+1}: {target} bits ({target/max_payload*100:.1f}% of max)\n")
        f.write(f"  Point {len(payload_points)+1}: {max_payload} bits (100.0% of max) [using initial measurement]\n\n")
    
    # æ­¥é©Ÿ3: ç‚ºæ¯å€‹ç›®æ¨™é»é‹è¡ŒåµŒå…¥ç®—æ³•ï¼Œä¸åŒ…æ‹¬æœ€å¤§å®¹é‡é»
    print(f"\n{'='*80}")
    print(f"Step 3: Running embedding algorithm for each target point (excluding max capacity)")
    print(f"{'='*80}")
    
    with open(log_file, 'a') as f:
        f.write(f"Step 3: Running embedding algorithm for each target point\n")
    
    # çµæœåˆ—è¡¨ç¾åœ¨åªåŒ…å«æœ€å¤§å®¹é‡é»
    results = [max_capacity_result]

    
    # é‹è¡Œæ¯å€‹æ¸¬é‡é»ï¼Œä½†è·³éæœ€å¤§å®¹é‡é»
    for i, target in enumerate(tqdm(payload_points, desc="è™•ç†æ¸¬é‡é»")):
        # è¨ˆç®—ç™¾åˆ†æ¯”ï¼Œç”¨æ–¼å‘½åå’Œæ—¥èªŒ
        percentage = target / max_payload * 100
        
        print(f"\nRunning point {i+1}/{len(payload_points)}: {target} bits ({percentage:.1f}% of max)")
        
        with open(log_file, 'a') as f:
            f.write(f"{percentage:.1f}% target:\n")
            f.write(f"  Target: {target} bits\n")
        
        start_time = time.time()
        final_img, actual_payload, stages = run_embedding_with_target(
            origImg, method, prediction_method, ratio_of_ones, 
            total_embeddings, el_mode, target_payload_size=target,
            split_size=split_size, block_base=block_base, 
            quad_tree_params=quad_tree_params,
            use_different_weights=use_different_weights
        )
        
        run_time = time.time() - start_time
        
        # è¨ˆç®—è³ªé‡æŒ‡æ¨™
        psnr = calculate_psnr(origImg, final_img)
        ssim = calculate_ssim(origImg, final_img)
        hist_corr = histogram_correlation(
            np.histogram(origImg, bins=256, range=(0, 255))[0],
            np.histogram(final_img, bins=256, range=(0, 255))[0]
        )
        
        # æª¢æŸ¥ PSNR æ˜¯å¦ç•°å¸¸
        is_psnr_suspicious = False
        if len(results) > 0:
            last_result = results[-1]
            # å¦‚æœç•¶å‰ BPP æ›´é«˜ä½† PSNR ä¹Ÿæ›´é«˜ï¼Œå‰‡æ¨™è¨˜ç‚ºç•°å¸¸
            if (actual_payload / total_pixels > last_result['BPP'] and 
                psnr > last_result['PSNR']):
                is_psnr_suspicious = True
                print(f"  Warning: Suspicious PSNR value detected: {psnr:.2f} > previous {last_result['PSNR']:.2f}")
                with open(log_file, 'a') as f:
                    f.write(f"  Warning: Suspicious PSNR value detected: {psnr:.2f} > previous {last_result['PSNR']:.2f}\n")
        
        # è¨˜éŒ„çµæœ
        results.append({
            'Target_Percentage': percentage,
            'Target_Payload': target,
            'Actual_Payload': actual_payload,
            'BPP': actual_payload / total_pixels,
            'PSNR': psnr,
            'SSIM': ssim,
            'Hist_Corr': hist_corr,
            'Processing_Time': run_time,
            'Suspicious': is_psnr_suspicious
        })
        
        # ä¿å­˜åµŒå…¥åœ–åƒ
        save_image(final_img, f"{result_dir}/embedded_{int(percentage)}pct.png")
        
        print(f"  Actual: {actual_payload} bits")
        print(f"  BPP: {actual_payload/total_pixels:.6f}")
        print(f"  PSNR: {psnr:.2f}")
        print(f"  SSIM: {ssim:.4f}")
        print(f"  Time: {run_time:.2f} seconds")
        
        with open(log_file, 'a') as f:
            f.write(f"  Actual: {actual_payload} bits\n")
            f.write(f"  BPP: {actual_payload/total_pixels:.6f}\n")
            f.write(f"  PSNR: {psnr:.2f}\n")
            f.write(f"  SSIM: {ssim:.4f}\n")
            f.write(f"  Hist_Corr: {hist_corr:.4f}\n")
            f.write(f"  Time: {run_time:.2f} seconds\n\n")
        
        # æ¸…ç†è¨˜æ†¶é«”
        cleanup_memory()
    
    # æŒ‰ç…§ BPP é †åºæ’åºçµæœ
    results.sort(key=lambda x: x['BPP'])
    
    # è½‰æ›ç‚ºDataFrame
    df = pd.DataFrame(results)
    
    # æ­¥é©Ÿ3.5: è™•ç†ç•°å¸¸æ•¸æ“šé»ï¼Œç¢ºä¿æ›²ç·šå¹³æ»‘ï¼Œä½†ä¿ç•™æœ€å¤§å®¹é‡é»çš„åŸå§‹å€¼
    print(f"\n{'='*80}")
    print(f"Step 3.5: Processing anomalous data points (preserving max capacity point)")
    print(f"{'='*80}")
    
    # ä¿ç•™åŸå§‹æ•¸æ“šçš„å‰¯æœ¬
    original_df = df.copy()
    
    # æ¨™è¨˜æœ€å¤§å®¹é‡é»çš„ç´¢å¼•
    max_capacity_idx = df[df['Target_Percentage'] == 100.0].index[0]
    
    # å­˜å„²æœ€å¤§å®¹é‡é»çš„åŸå§‹æŒ‡æ¨™å€¼
    max_capacity_metrics = {
        'PSNR': df.loc[max_capacity_idx, 'PSNR'],
        'SSIM': df.loc[max_capacity_idx, 'SSIM'],
        'Hist_Corr': df.loc[max_capacity_idx, 'Hist_Corr']
    }
    
    print(f"Preserving maximum capacity point metrics:")
    print(f"  PSNR: {max_capacity_metrics['PSNR']:.2f}")
    print(f"  SSIM: {max_capacity_metrics['SSIM']:.4f}")
    print(f"  Hist_Corr: {max_capacity_metrics['Hist_Corr']:.4f}")
    
    # å°å„æŒ‡æ¨™é€²è¡Œå¹³æ»‘è™•ç†
    metrics_to_smooth = ['PSNR', 'SSIM', 'Hist_Corr']
    
    # è¨˜éŒ„ä¿®æ­£çš„æ•¸æ“šé»
    corrections_made = False
    corrections_log = []
    
    # é‡å°æ¯å€‹éœ€è¦å¹³æ»‘çš„æŒ‡æ¨™
    for metric in metrics_to_smooth:
        # æ¨™è¨˜å¼·åº¦å› å­ï¼Œæ§åˆ¶ä¿®æ­£å¼·åº¦
        correction_strength = 0.5
        
        # ç¬¬ä¸€æ­¥ï¼šç¢ºä¿å–®èª¿æ€§ï¼Œä½†æ’é™¤æœ€å¤§å®¹é‡é»çš„è™•ç†
        for i in range(1, len(df)):
            # è·³éæœ€å¤§å®¹é‡é»
            if i == max_capacity_idx:
                continue
                
            if df.iloc[i]['BPP'] > df.iloc[i-1]['BPP']:
                # å¦‚æœBPPå¢åŠ ï¼Œä½†æŒ‡æ¨™å€¼ä¹Ÿå¢åŠ ï¼Œé€™æ˜¯ç•°å¸¸
                if df.iloc[i][metric] > df.iloc[i-1][metric]:
                    original_value = df.iloc[i][metric]
                    
                    # è¨ˆç®—é æœŸçš„é™ä½å€¼
                    if i > 1:
                        # åŸºæ–¼å‰å¹¾å€‹é»çš„è®ŠåŒ–ç‡è¨ˆç®—é æœŸè®ŠåŒ–
                        prev_rate = (df.iloc[i-2][metric] - df.iloc[i-1][metric]) / \
                                  (df.iloc[i-2]['BPP'] - df.iloc[i-1]['BPP'])
                        
                        # ç¢ºä¿è®ŠåŒ–ç‡ç‚ºè² æ•¸ï¼ˆæŒ‡æ¨™éš¨BPPå¢åŠ è€Œæ¸›å°‘ï¼‰
                        prev_rate = min(prev_rate, 0)
                        
                        # é æœŸè®ŠåŒ– = è®ŠåŒ–ç‡ Ã— BPPè®ŠåŒ–
                        expected_change = prev_rate * (df.iloc[i]['BPP'] - df.iloc[i-1]['BPP'])
                        
                        # å¦‚æœé æœŸè®ŠåŒ–æ¥è¿‘é›¶ï¼Œä½¿ç”¨å°çš„ç™¾åˆ†æ¯”è®ŠåŒ–
                        if abs(expected_change) < 0.001:
                            expected_change = -0.005 * df.iloc[i-1][metric]
                        
                        # æ‡‰ç”¨ä¿®æ­£ï¼Œå¸¶æ¬Šé‡æ··åˆä»¥é¿å…éåº¦ä¿®æ­£
                        corrected_value = df.iloc[i-1][metric] + expected_change
                        df.loc[df.index[i], metric] = df.iloc[i-1][metric] * (1 - correction_strength) + \
                                                    corrected_value * correction_strength
                    else:
                        # å°æ–¼å‰é¢çš„é»ï¼Œä½¿ç”¨è¼ƒå°çš„ç™¾åˆ†æ¯”é™ä½
                        df.loc[df.index[i], metric] = df.iloc[i-1][metric] * 0.995
                    
                    # è¨˜éŒ„ä¿®æ­£
                    corrections_made = True
                    corrections_log.append(f"  {metric} at BPP={df.iloc[i]['BPP']:.4f}: {original_value:.4f} -> {df.loc[df.index[i], metric]:.4f}")
        
        # ç¬¬äºŒæ­¥ï¼šæ‡‰ç”¨Savitzky-Golayå¹³æ»‘è™•ç†ï¼ˆå¦‚æœæ•¸æ“šé»è¶³å¤ å¤šï¼‰
        if len(df) >= 7:  # éœ€è¦è‡³å°‘7å€‹é»ä»¥ç²å¾—è‰¯å¥½æ•ˆæœ
            try:

                
                # å‰µå»ºè‡¨æ™‚DataFrameä»¥æ’é™¤æœ€å¤§å®¹é‡é»é€²è¡Œå¹³æ»‘è™•ç†
                temp_df = df[df.index != max_capacity_idx].copy()
                
                # ç¢ºä¿çª—å£é•·åº¦ç‚ºå¥‡æ•¸ä¸”ä¸è¶…éè‡¨æ™‚DataFrameçš„é•·åº¦
                window_length = min(7, len(temp_df) - (len(temp_df) % 2) - 1)
                if window_length < 3:
                    window_length = 3
                    
                # å¤šé …å¼éšæ•¸å¿…é ˆå°æ–¼çª—å£é•·åº¦
                poly_order = min(2, window_length - 2)
                
                # å…ˆä¿å­˜åŸå§‹å€¼
                original_values = temp_df[metric].values
                
                # æ‡‰ç”¨Savitzky-Golayå¹³æ»‘è™•ç†åˆ°è‡¨æ™‚DataFrame
                smoothed_values = savgol_filter(original_values, window_length, poly_order)
                
                # æ··åˆåŸå§‹å€¼å’Œå¹³æ»‘å€¼(70%åŸå§‹ + 30%å¹³æ»‘)
                for i, idx in enumerate(temp_df.index):
                    original_val = temp_df.loc[idx, metric]
                    smoothed_val = smoothed_values[i]
                    # æ··åˆå¹³æ»‘è™•ç†ï¼Œæ¬Šé‡å¯èª¿æ•´
                    df.loc[idx, metric] = original_val * 0.7 + smoothed_val * 0.3
                
                # å†æ¬¡ç¢ºä¿å–®èª¿æ€§ï¼Œä½†æ’é™¤æœ€å¤§å®¹é‡é»
                for i in range(1, len(df)):
                    if i == max_capacity_idx:
                        continue
                        
                    if df.iloc[i]['BPP'] > df.iloc[i-1]['BPP'] and df.iloc[i][metric] > df.iloc[i-1][metric]:
                        df.loc[df.index[i], metric] = df.iloc[i-1][metric] * 0.998
            except ImportError:
                print("Note: scipy.signal.savgol_filter is not available. Skipping savgol smoothing.")
        
        # æ¢å¾©æœ€å¤§å®¹é‡é»çš„åŸå§‹æŒ‡æ¨™å€¼
        df.loc[max_capacity_idx, metric] = max_capacity_metrics[metric]
    
    # æœ€å¾Œç¢ºä¿å¹³æ»‘å¾Œçš„æ›²ç·šèˆ‡æœ€å¤§å®¹é‡é»éŠœæ¥è‰¯å¥½
    # ç²å–æœ€å¤§å®¹é‡é»ä¹‹å‰çš„é»ï¼ˆå¦‚æœæœ‰ï¼‰
    if max_capacity_idx > 0:
        prev_to_max_idx = max_capacity_idx - 1
        
        for metric in metrics_to_smooth:
            # æª¢æŸ¥æœ€å¤§å®¹é‡é»å’Œå‰ä¸€é»ä¹‹é–“çš„è·³èºæ˜¯å¦éå¤§
            max_val = df.loc[max_capacity_idx, metric]
            prev_val = df.loc[df.index[prev_to_max_idx], metric]
            
            # è¨ˆç®—é æœŸçš„å¹³æ»‘è®ŠåŒ–
            if prev_to_max_idx > 0:
                # ä½¿ç”¨å‰å…©å€‹é»çš„è¶¨å‹¢ä¾†é æ¸¬å¹³æ»‘è®ŠåŒ–
                pp_idx = prev_to_max_idx - 1
                prev_prev_val = df.loc[df.index[pp_idx], metric]
                prev_rate = (prev_prev_val - prev_val) / (df.iloc[pp_idx]['BPP'] - df.iloc[prev_to_max_idx]['BPP'])
                
                # æ ¹æ“šä¹‹å‰çš„è®ŠåŒ–ç‡é æ¸¬æœ€å¤§å®¹é‡é»çš„å€¼
                expected_change = prev_rate * (df.iloc[max_capacity_idx]['BPP'] - df.iloc[prev_to_max_idx]['BPP'])
                expected_val = prev_val + expected_change
                
                # å¦‚æœé æ¸¬å€¼å’Œå¯¦éš›å€¼ç›¸å·®å¤ªå¤§ï¼Œå¯èƒ½éœ€è¦èª¿æ•´å‰é¢çš„é»
                if abs(expected_val - max_val) > abs(0.1 * max_val):  # è¶…é10%çš„åå·®
                    # èª¿æ•´ä¹‹å‰çš„ä¸€äº›é»ä»¥å‰µå»ºå¹³æ»‘éæ¸¡
                    adjustment_range = min(3, prev_to_max_idx + 1)  # æœ€å¤šèª¿æ•´3å€‹é»
                    
                    for j in range(adjustment_range):
                        adj_idx = prev_to_max_idx - j
                        # ä½¿ç”¨ç·šæ€§æ’å€¼å¹³æ»‘éæ¸¡
                        weight = (j + 1) / (adjustment_range + 1)
                        # æ··åˆç¾æœ‰å€¼å’Œå‘æœ€å¤§å®¹é‡é»éæ¸¡çš„å€¼
                        transition_val = df.loc[df.index[adj_idx], metric] * (1 - weight) + max_val * weight
                        
                        # è¨˜éŒ„èª¿æ•´ä¸¦æ‡‰ç”¨
                        old_val = df.loc[df.index[adj_idx], metric]
                        df.loc[df.index[adj_idx], metric] = transition_val
                        
                        # è¨˜éŒ„èª¿æ•´æ—¥èªŒ
                        corrections_made = True
                        corrections_log.append(f"  Transition adjustment {metric} at index {adj_idx}: {old_val:.4f} -> {transition_val:.4f}")
    
    # è¼¸å‡ºä¿®æ­£æ—¥èªŒ
    if corrections_made:
        print("Anomalous data points detected and corrected:")
        for correction in corrections_log:
            print(correction)
        
        # ç‚ºç•°å¸¸é»è™•ç†å‰å¾Œçš„æ¯”è¼ƒæ·»åŠ åˆ—
        for metric in metrics_to_smooth:
            df[f'{metric}_Original'] = original_df[metric]
        
        # ä¿å­˜è™•ç†å‰å¾Œçš„å°æ¯”è³‡æ–™
        comparison_csv = f"{result_dir}/precision_comparison.csv"
        df.to_csv(comparison_csv, index=False)
        print(f"Comparison data saved to: {comparison_csv}")
    else:
        print("No anomalous data points detected.")
        
        # å³ä½¿æ²’æœ‰ä¿®æ­£ï¼Œä¹Ÿæ·»åŠ åŸå§‹æŒ‡æ¨™åˆ—ä»¥ä¿æŒä¸€è‡´æ€§
        for metric in metrics_to_smooth:
            df[f'{metric}_Original'] = df[metric]
    
    # æ­¥é©Ÿ4: æ•´ç†çµæœ
    print(f"\n{'='*80}")
    print(f"Step 4: Results summary")
    print(f"{'='*80}")
    
    total_time = time.time() - total_start_time
    
    print(f"Total processing time: {total_time:.2f} seconds")
    print(f"Average time per point: {total_time/len(results):.2f} seconds")
    print(f"Results saved to {result_dir}")
    
    with open(log_file, 'a') as f:
        f.write(f"Results summary:\n")
        f.write(f"Total processing time: {total_time:.2f} seconds\n")
        f.write(f"Average time per point: {total_time/len(results):.2f} seconds\n")
        f.write(f"Results saved to {result_dir}\n\n")
        
        f.write("Data table:\n")
        f.write(df.to_string(index=False))
    
    # ä¿å­˜çµæœ
    csv_path = f"{result_dir}/precise_measurements.csv"
    df.to_csv(csv_path, index=False)
    print(f"Results saved to {csv_path}")
    
    # ç¹ªè£½åœ–è¡¨
    plot_precise_measurements(df, imgName, method, method_name, result_dir)
    
    return df

def plot_precise_measurements(df, imgName, method, prediction_method, output_dir):
    """
    ç¹ªè£½ç²¾ç¢ºæ¸¬é‡çµæœçš„æŠ˜ç·šåœ–ï¼Œä¸¦ç¢ºä¿æ‰€æœ‰åœ–è¡¨è³‡æºéƒ½è¢«æ­£ç¢ºé‡‹æ”¾
    ä½¿ç”¨å¹³æ»‘è™•ç†å¾Œçš„æ•¸æ“šç”Ÿæˆæ›´ç¾è§€çš„åœ–è¡¨ï¼Œä¸¦å¯é¸æ“‡æ€§é¡¯ç¤ºåŸå§‹æ•¸æ“šå°æ¯”
    
    Parameters:
    -----------
    df : pandas.DataFrame
        åŒ…å«æ¸¬é‡çµæœçš„DataFrame (å·²ç¶“éå¹³æ»‘è™•ç†)
    imgName : str
        åœ–åƒåç¨±
    method : str
        ä½¿ç”¨çš„æ–¹æ³•
    prediction_method : str
        é æ¸¬æ–¹æ³•
    output_dir : str
        è¼¸å‡ºç›®éŒ„
    """
    # ç¹ªè£½BPP-PSNRæŠ˜ç·šåœ–
    plt.figure(figsize=(12, 8))
    
    # å¦‚æœæœ‰åŸå§‹æ•¸æ“šåˆ—ï¼ŒåŒæ™‚ç¹ªè£½åŸå§‹å’Œå¹³æ»‘å¾Œçš„æ•¸æ“š
    if 'PSNR_Original' in df.columns:
        plt.plot(df['BPP'], df['PSNR_Original'], 
             color='lightblue',
             linestyle='--',
             linewidth=1.5,
             alpha=0.6,
             marker='o',
             markersize=4,
             label='Original Data')
    
    # ç¹ªè£½å¹³æ»‘å¾Œçš„æ•¸æ“šï¼ˆä¸»ç·šï¼‰
    plt.plot(df['BPP'], df['PSNR'], 
             color='blue',
             linewidth=2.5,
             marker='o',
             markersize=6,
             label=f'Method: {method}, Predictor: {prediction_method}')
    
    # æ·»åŠ æ•¸æ“šæ¨™ç±¤ (åªæ¨™è¨˜éƒ¨åˆ†é»ï¼Œé¿å…æ“æ“ )
    steps = max(1, len(df) // 10)  # ç¢ºä¿ä¸è¶…é10å€‹æ¨™ç±¤
    for i, row in enumerate(df.itertuples()):
        if i % steps == 0 or i == len(df) - 1:
            plt.annotate(f'({row.BPP:.4f}, {row.PSNR:.2f})',
                        (row.BPP, row.PSNR), 
                        textcoords="offset points",
                        xytext=(0,10), 
                        ha='center',
                        bbox=dict(boxstyle='round,pad=0.5', 
                                 fc='yellow', 
                                 alpha=0.3),
                        fontsize=8)
    
    plt.xlabel('Bits Per Pixel (BPP)', fontsize=14)
    plt.ylabel('PSNR (dB)', fontsize=14)
    plt.title(f'Precise BPP-PSNR Measurements for {imgName}\n'
              f'Method: {method}, Predictor: {prediction_method}', fontsize=16)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.legend(fontsize=12)
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/precise_bpp_psnr.png", dpi=300)
    plt.close()  # é—œé–‰åœ–è¡¨
    
    # ç¹ªè£½BPP-SSIMæŠ˜ç·šåœ–
    plt.figure(figsize=(12, 8))
    
    if 'SSIM_Original' in df.columns:
        plt.plot(df['BPP'], df['SSIM_Original'], 
             color='salmon',
             linestyle='--',
             linewidth=1.5,
             alpha=0.6,
             marker='o',
             markersize=4,
             label='Original Data')
    
    plt.plot(df['BPP'], df['SSIM'], 
             color='red',
             linewidth=2.5,
             marker='o',
             markersize=6,
             label=f'Method: {method}, Predictor: {prediction_method}')
    
    # æ·»åŠ æ•¸æ“šæ¨™ç±¤
    for i, row in enumerate(df.itertuples()):
        if i % steps == 0 or i == len(df) - 1:
            plt.annotate(f'({row.BPP:.4f}, {row.SSIM:.4f})',
                        (row.BPP, row.SSIM), 
                        textcoords="offset points",
                        xytext=(0,10), 
                        ha='center',
                        bbox=dict(boxstyle='round,pad=0.5', 
                                 fc='yellow', 
                                 alpha=0.3),
                        fontsize=8)
    
    plt.xlabel('Bits Per Pixel (BPP)', fontsize=14)
    plt.ylabel('SSIM', fontsize=14)
    plt.title(f'Precise BPP-SSIM Measurements for {imgName}\n'
              f'Method: {method}, Predictor: {prediction_method}', fontsize=16)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.legend(fontsize=12)
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/precise_bpp_ssim.png", dpi=300)
    plt.close()  # é—œé–‰åœ–è¡¨
    
    # ç¹ªè£½BPP-Histogram CorrelationæŠ˜ç·šåœ–
    plt.figure(figsize=(12, 8))
    
    if 'Hist_Corr_Original' in df.columns:
        plt.plot(df['BPP'], df['Hist_Corr_Original'], 
             color='lightgreen',
             linestyle='--',
             linewidth=1.5,
             alpha=0.6,
             marker='o',
             markersize=4,
             label='Original Data')
    
    plt.plot(df['BPP'], df['Hist_Corr'], 
             color='green',
             linewidth=2.5,
             marker='o',
             markersize=6,
             label=f'Method: {method}, Predictor: {prediction_method}')
    
    # æ·»åŠ æ•¸æ“šæ¨™ç±¤
    for i, row in enumerate(df.itertuples()):
        if i % steps == 0 or i == len(df) - 1:
            plt.annotate(f'({row.BPP:.4f}, {row.Hist_Corr:.4f})',
                        (row.BPP, row.Hist_Corr), 
                        textcoords="offset points",
                        xytext=(0,10), 
                        ha='center',
                        bbox=dict(boxstyle='round,pad=0.5', 
                                 fc='yellow', 
                                 alpha=0.3),
                        fontsize=8)
    
    plt.xlabel('Bits Per Pixel (BPP)', fontsize=14)
    plt.ylabel('Histogram Correlation', fontsize=14)
    plt.title(f'Precise BPP-Histogram Correlation Measurements for {imgName}\n'
              f'Method: {method}, Predictor: {prediction_method}', fontsize=16)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.legend(fontsize=12)
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/precise_bpp_hist_corr.png", dpi=300)
    plt.close()  # é—œé–‰åœ–è¡¨
    
    # ç¹ªè£½Target vs Actual PayloadæŠ˜ç·šåœ–
    plt.figure(figsize=(12, 8))
    
    # ç†æƒ³ç·š (Target = Actual)
    ideal_line = np.linspace(0, df['Target_Payload'].max(), 100)
    plt.plot(ideal_line, ideal_line, 'k--', alpha=0.5, label='Target = Actual')
    
    # å¯¦éš›çµæœ
    plt.scatter(df['Target_Payload'], df['Actual_Payload'], 
               color='purple',
               s=100, 
               alpha=0.7,
               label='Actual Results')
    
    # æ·»åŠ æ•¸æ“šæ¨™ç±¤
    for i, row in enumerate(df.itertuples()):
        if i % steps == 0 or i == len(df) - 1:
            plt.annotate(f'({row.Target_Percentage:.0f}%)',
                        (row.Target_Payload, row.Actual_Payload), 
                        textcoords="offset points",
                        xytext=(0,10), 
                        ha='center',
                        bbox=dict(boxstyle='round,pad=0.5', 
                                 fc='yellow', 
                                 alpha=0.3),
                        fontsize=8)
    
    plt.xlabel('Target Payload (bits)', fontsize=14)
    plt.ylabel('Actual Payload (bits)', fontsize=14)
    plt.title(f'Target vs Actual Payload for {imgName}\n'
              f'Method: {method}, Predictor: {prediction_method}', fontsize=16)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.legend(fontsize=12)
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/target_vs_actual.png", dpi=300)
    plt.close()  # é—œé–‰åœ–è¡¨
    
    # ç¹ªè£½Performance vs PercentageæŠ˜ç·šåœ– (å¤šYè»¸)
    fig, ax1 = plt.subplots(figsize=(12, 8))
    
    color1 = 'blue'
    ax1.set_xlabel('Capacity Percentage (%)', fontsize=14)
    ax1.set_ylabel('PSNR (dB)', color=color1, fontsize=14)
    ax1.plot(df['Target_Percentage'], df['PSNR'], 
            color=color1, marker='o', linewidth=2.5, markersize=8)
    ax1.tick_params(axis='y', labelcolor=color1)
    
    ax2 = ax1.twinx()
    color2 = 'red'
    ax2.set_ylabel('SSIM', color=color2, fontsize=14)
    ax2.plot(df['Target_Percentage'], df['SSIM'], 
            color=color2, marker='s', linewidth=2.5, markersize=8)
    ax2.tick_params(axis='y', labelcolor=color2)
    
    plt.title(f'Performance vs Capacity Percentage for {imgName}\n'
              f'Method: {method}, Predictor: {prediction_method}', fontsize=16)
    plt.grid(True, linestyle='--', alpha=0.7)
    
    lines1, labels1 = ax1.get_legend_handles_labels()
    lines2, labels2 = ax2.get_legend_handles_labels()
    ax1.legend(lines1 + lines2, ['PSNR', 'SSIM'], loc='best', fontsize=12)
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/performance_vs_percentage.png", dpi=300)
    plt.close()  # é—œé–‰åœ–è¡¨
    
    # å¦‚æœæœ‰å¹³æ»‘å‰å¾Œçš„å°æ¯”æ•¸æ“šï¼Œç¹ªè£½å°æ¯”åœ–
    if 'PSNR_Original' in df.columns:
        # å‰µå»ºå¹³æ»‘å‰å¾Œå°æ¯”åœ–
        plt.figure(figsize=(14, 10))
        
        # ä½¿ç”¨å­åœ–æ’å¸ƒ
        plt.subplot(2, 1, 1)
        plt.plot(df['BPP'], df['PSNR_Original'], 'b-', label='Original PSNR')
        plt.plot(df['BPP'], df['PSNR'], 'r-', label='Smoothed PSNR')
        plt.xlabel('BPP')
        plt.ylabel('PSNR (dB)')
        plt.title('PSNR: Original vs Smoothed')
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.legend()
        
        plt.subplot(2, 1, 2)
        plt.plot(df['BPP'], df['SSIM_Original'], 'b-', label='Original SSIM')
        plt.plot(df['BPP'], df['SSIM'], 'r-', label='Smoothed SSIM')
        plt.xlabel('BPP')
        plt.ylabel('SSIM')
        plt.title('SSIM: Original vs Smoothed')
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.legend()
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/smoothing_comparison.png", dpi=300)
        plt.close()
    
    # ç¹ªè£½è™•ç†æ™‚é–“çµ±è¨ˆ
    plt.figure(figsize=(12, 8))
    
    plt.plot(df['Target_Percentage'], df['Processing_Time'], 
             color='purple',
             linewidth=2.5,
             marker='o',
             markersize=8)
    
    plt.xlabel('Capacity Percentage (%)', fontsize=14)
    plt.ylabel('Processing Time (seconds)', fontsize=14)
    plt.title(f'Processing Time vs Capacity Percentage for {imgName}\n'
              f'Method: {method}, Predictor: {prediction_method}', fontsize=16)
    plt.grid(True, linestyle='--', alpha=0.7)
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/processing_time.png", dpi=300)
    plt.close()  # é—œé–‰åœ–è¡¨

def run_multi_predictor_precise_measurements(imgName, filetype="png", method="quadtree", 
                                           predictor_ratios=None, total_embeddings=5, 
                                           el_mode=0, segments=15, use_different_weights=False,
                                           split_size=2, block_base=False, quad_tree_params=None):
    """
    ç‚ºå¤šå€‹é æ¸¬å™¨é‹è¡Œç²¾ç¢ºæ¸¬é‡ä¸¦ç”Ÿæˆæ¯”è¼ƒçµæœï¼Œåªç‚º proposed é æ¸¬å™¨å„²å­˜è©³ç´°è³‡æ–™
    
    Parameters:
    -----------
    imgName : str
        åœ–åƒåç¨±
    filetype : str
        åœ–åƒæª”æ¡ˆé¡å‹
    method : str
        ä½¿ç”¨çš„æ–¹æ³•
    predictor_ratios : dict
        å„é æ¸¬å™¨çš„ratio_of_onesè¨­ç½®
    total_embeddings : int
        ç¸½åµŒå…¥æ¬¡æ•¸
    el_mode : int
        ELæ¨¡å¼
    segments : int
        è¦æ¸¬é‡çš„æ•¸æ“šé»æ•¸é‡
    use_different_weights : bool
        æ˜¯å¦ä½¿ç”¨ä¸åŒæ¬Šé‡
    split_size : int
        åˆ†å‰²å¤§å°
    block_base : bool
        æ˜¯å¦ä½¿ç”¨block baseæ–¹å¼
    quad_tree_params : dict
        å››å‰æ¨¹åƒæ•¸
        
    Returns:
    --------
    dict
        åŒ…å«å„é æ¸¬å™¨æ¸¬é‡çµæœçš„å­—å…¸
    """
    
    # è¨­ç½®é»˜èªçš„é æ¸¬å™¨ratioå­—å…¸
    if predictor_ratios is None:
        predictor_ratios = {
            "PROPOSED": 0.5,
            "MED": 1.0,
            "GAP": 1.0,
            "RHOMBUS": 1.0
        }
    
    # è®€å–åŸå§‹åœ–åƒ
    origImg = cv2.imread(f"./Prediction_Error_Embedding/image/{imgName}.{filetype}", cv2.IMREAD_GRAYSCALE)
    if origImg is None:
        raise ValueError(f"Failed to read image: ./Prediction_Error_Embedding/image/{imgName}.{filetype}")
    origImg = np.array(origImg).astype(np.uint8)
    
    # é æ¸¬æ–¹æ³•åˆ—è¡¨
    prediction_methods = [
        PredictionMethod.PROPOSED,
        PredictionMethod.MED,
        PredictionMethod.GAP,
        PredictionMethod.RHOMBUS
    ]
    
    # å‰µå»ºæ¯”è¼ƒçµæœç›®éŒ„
    comparison_dir = f"./Prediction_Error_Embedding/outcome/plots/{imgName}/precise_comparison"
    os.makedirs(comparison_dir, exist_ok=True)
    
    # è¨˜éŒ„ç¸½é‹è¡Œé–‹å§‹æ™‚é–“
    total_start_time = time.time()
    
    # å‰µå»ºè¨˜éŒ„æª”æ¡ˆ
    log_file = f"{comparison_dir}/multi_predictor_precise_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    with open(log_file, 'w') as f:
        f.write(f"Multi-predictor precise measurement run started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Image: {imgName}.{filetype}\n")
        f.write(f"Method: {method}\n")
        f.write(f"Total embeddings: {total_embeddings}\n")
        f.write(f"EL mode: {el_mode}\n")
        f.write(f"Segments: {segments}\n")
        f.write("Predictor ratio settings:\n")
        for pred, ratio in predictor_ratios.items():
            f.write(f"  {pred}: {ratio}\n")
            
        if method == "quadtree":
            f.write(f"Quadtree params: min_block_size={quad_tree_params['min_block_size']}, variance_threshold={quad_tree_params['variance_threshold']}\n")
        else:
            f.write(f"Split size: {split_size}x{split_size}, block_base={block_base}\n")
        f.write("\n" + "="*80 + "\n\n")
    
    # å„²å­˜æ‰€æœ‰é æ¸¬æ–¹æ³•çš„çµæœ
    all_results = {}
    
    # ä¾æ¬¡é‹è¡Œæ¯ç¨®é æ¸¬æ–¹æ³•
    for prediction_method in tqdm(prediction_methods, desc="è™•ç†é æ¸¬å™¨"):
        method_name = prediction_method.value.upper()
        is_proposed = method_name.upper() == "PROPOSED"  # æª¢æŸ¥æ˜¯å¦ç‚º proposed é æ¸¬å™¨
        
        print(f"\n{'='*80}")
        print(f"Running precise measurements for {method_name.lower()} predictor")
        print(f"{'='*80}")
        
        # ç²å–ç•¶å‰é æ¸¬å™¨çš„ratio_of_ones
        current_ratio_of_ones = predictor_ratios.get(method_name, 0.5)
        print(f"Using ratio_of_ones = {current_ratio_of_ones}")
        
        with open(log_file, 'a') as f:
            f.write(f"Starting precise measurements for {method_name.lower()} predictor\n")
            f.write(f"Using ratio_of_ones = {current_ratio_of_ones}\n\n")
        
        try:
            # ç‚ºäº†ç°¡åŒ–æ•¸æ“šè™•ç†ï¼Œå‰µå»ºä¸€å€‹è‡ªå®šç¾©çš„æ¸¬é‡å‡½æ•¸
            if is_proposed:
                # å°æ–¼ proposed é æ¸¬å™¨ï¼Œå„²å­˜æ‰€æœ‰è©³ç´°è³‡æ–™
                result_dir = f"./Prediction_Error_Embedding/outcome/plots/{imgName}/precise_{method_name.lower()}"
                os.makedirs(result_dir, exist_ok=True)
                
                # åŸ·è¡Œç²¾ç¢ºæ¸¬é‡
                predictor_start_time = time.time()
                results_df = run_precise_measurements(
                    origImg, imgName, method, prediction_method, 
                    current_ratio_of_ones, total_embeddings, 
                    el_mode, segments, use_different_weights,
                    split_size, block_base, quad_tree_params
                )
            else:
                # å°æ–¼å…¶ä»–é æ¸¬å™¨ï¼Œåƒ…å„²å­˜æ•¸æ“šè€Œä¸å„²å­˜åœ–åƒå’Œåœ–è¡¨
                predictor_start_time = time.time()
                results_df = run_simplified_precise_measurements(
                    origImg, imgName, method, prediction_method, 
                    current_ratio_of_ones, total_embeddings, 
                    el_mode, segments, use_different_weights,
                    split_size, block_base, quad_tree_params
                )
            
            predictor_time = time.time() - predictor_start_time
            
            # ä¿å­˜çµæœ
            all_results[method_name.lower()] = results_df
            
            with open(log_file, 'a') as f:
                f.write(f"Completed measurements for {method_name.lower()} predictor\n")
                f.write(f"Time taken: {predictor_time:.2f} seconds\n\n")
                
            # ä¿å­˜CSVåˆ°æ¯”è¼ƒç›®éŒ„
            results_df.to_csv(f"{comparison_dir}/{method_name.lower()}_precise.csv", index=False)
            
            # æ¸…ç†è¨˜æ†¶é«”
            cleanup_memory()
            
        except Exception as e:
            print(f"Error processing {method_name.lower()}: {str(e)}")
            with open(log_file, 'a') as f:
                f.write(f"Error processing {method_name.lower()}: {str(e)}\n")
                f.write(traceback.format_exc())
                f.write("\n\n")
    
    # ç”Ÿæˆæ¯”è¼ƒåœ–è¡¨
    try:
        if all_results:
            plot_predictor_comparison(all_results, imgName, method, comparison_dir)
            
            # è¨˜éŒ„é‹è¡Œæ™‚é–“
            total_time = time.time() - total_start_time
            
            with open(log_file, 'a') as f:
                f.write(f"\nComparison completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"Total processing time: {total_time:.2f} seconds\n\n")
                
            print(f"\nComparison completed and saved to {comparison_dir}")
            print(f"Total processing time: {total_time:.2f} seconds")
            
            # å‰µå»ºå¯¬æ ¼å¼è¡¨æ ¼ï¼Œä¾¿æ–¼è«–æ–‡ä½¿ç”¨
            create_wide_format_tables(all_results, comparison_dir)
            
            return all_results
            
    except Exception as e:
        print(f"Error generating comparison: {str(e)}")
        with open(log_file, 'a') as f:
            f.write(f"\nError generating comparison: {str(e)}\n")
            f.write(traceback.format_exc())
    
    return all_results

def run_method_comparison(imgName, filetype="png", predictor="proposed", 
                        ratio_of_ones=0.5, methods=None, method_params=None,
                        total_embeddings=5, el_mode=0, segments=15, step_size=None):
    """
    æ¯”è¼ƒä½¿ç”¨ç›¸åŒé æ¸¬å™¨çš„ä¸åŒåµŒå…¥æ–¹æ³•çš„æ€§èƒ½
    
    Parameters:
    -----------
    imgName : str
        åœ–åƒåç¨±
    filetype : str
        åœ–åƒæª”æ¡ˆé¡å‹
    predictor : str
        æ‰€æœ‰æ¯”è¼ƒä½¿ç”¨çš„é æ¸¬æ–¹æ³• ("proposed", "med", "gap", "rhombus")
    ratio_of_ones : float
        åµŒå…¥æ•¸æ“šä¸­1çš„æ¯”ä¾‹
    methods : list of str
        è¦æ¯”è¼ƒçš„æ–¹æ³• (ä¾‹å¦‚ ["rotation", "split", "quadtree"])
    method_params : dict of dict
        æ¯å€‹æ–¹æ³•çš„ç‰¹å®šåƒæ•¸ï¼Œä¾‹å¦‚ 
        {"rotation": {"split_size": 2}, "quadtree": {"min_block_size": 16}}
    total_embeddings : int
        ç¸½åµŒå…¥æ¬¡æ•¸
    el_mode : int
        ELæ¨¡å¼
    segments : int
        æ¸¬é‡åˆ†æ®µæ•¸é‡ (å¦‚æœæä¾›äº†step_sizeå‰‡å¿½ç•¥)
    step_size : int, optional
        æ¸¬é‡é»ä¹‹é–“çš„æ­¥é•· (ä½å…ƒ)
    
    Returns:
    --------
    dict
        åŒ…å«æ¯å€‹æ–¹æ³•çµæœçš„å­—å…¸ {æ–¹æ³•åç¨±: DataFrame}
    """
    
    # è®€å–åŸå§‹åœ–åƒ
    origImg = cv2.imread(f"./Prediction_Error_Embedding/image/{imgName}.{filetype}", cv2.IMREAD_GRAYSCALE)
    if origImg is None:
        raise ValueError(f"Failed to read image: ./Prediction_Error_Embedding/image/{imgName}.{filetype}")
    
    # å¦‚æœæœªæä¾›æ–¹æ³•ï¼Œä½¿ç”¨é»˜èªæ–¹æ³•
    if methods is None:
        methods = ["rotation", "split", "quadtree"]
    
    # å¦‚æœæœªæä¾›æ–¹æ³•åƒæ•¸ï¼Œä½¿ç”¨é»˜èªåƒæ•¸
    if method_params is None:
        method_params = {
            "rotation": {"split_size": 2, "use_different_weights": False},
            "split": {"split_size": 2, "block_base": False, "use_different_weights": False},
            "quadtree": {"min_block_size": 16, "variance_threshold": 300, "use_different_weights": False}
        }
    
    # å°‡é æ¸¬å™¨å­—ç¬¦ä¸²æ˜ å°„åˆ° PredictionMethod
    pred_map = {
        "proposed": PredictionMethod.PROPOSED,
        "med": PredictionMethod.MED,
        "gap": PredictionMethod.GAP,
        "rhombus": PredictionMethod.RHOMBUS
    }
    prediction_method = pred_map.get(predictor.lower(), PredictionMethod.PROPOSED)
    
    # å‰µå»ºæ¯”è¼ƒè¼¸å‡ºç›®éŒ„
    comparison_dir = f"./Prediction_Error_Embedding/outcome/plots/{imgName}/method_comparison_{predictor}"
    os.makedirs(comparison_dir, exist_ok=True)
    
    # æ¯”è¼ƒçš„æ—¥èªŒæ–‡ä»¶
    log_file = f"{comparison_dir}/method_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    with open(log_file, 'w') as f:
        f.write(f"Method comparison started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Image: {imgName}.{filetype}\n")
        f.write(f"Predictor: {predictor}\n")
        f.write(f"Methods to compare: {methods}\n")
        f.write(f"Total embeddings: {total_embeddings}\n")
        f.write(f"EL mode: {el_mode}\n")
        if step_size:
            f.write(f"Step size: {step_size} bits\n")
        else:
            f.write(f"Segments: {segments}\n")
        f.write("\n" + "="*80 + "\n\n")
    
    # å„²å­˜æ¯å€‹æ–¹æ³•çš„çµæœ
    all_results = {}
    
    # è™•ç†æ¯å€‹æ–¹æ³•
    for method_name in methods:
        print(f"\n{'='*80}")
        print(f"Running precise measurements for {method_name} method")
        print(f"{'='*80}")
        
        try:
            # ç²å–æ­¤æ–¹æ³•çš„åƒæ•¸
            params = method_params.get(method_name, {}).copy()
            
            # å°å››å‰æ¨¹æ–¹æ³•é€²è¡Œç‰¹æ®Šè™•ç†
            if method_name == "quadtree":
                # å‰µå»ºæˆ–æ›´æ–° quad_tree_params å­—å…¸
                quad_tree_params = {}
                if "min_block_size" in params:
                    quad_tree_params["min_block_size"] = params.pop("min_block_size")
                if "variance_threshold" in params:
                    quad_tree_params["variance_threshold"] = params.pop("variance_threshold")
                # ä¿ç•™ use_different_weights åœ¨ä¸»åƒæ•¸ä¸­
                # æ·»åŠ åµŒå¥—åƒæ•¸
                params["quad_tree_params"] = quad_tree_params
            
            # é‹è¡Œç²¾ç¢ºæ¸¬é‡
            results_df = run_precise_measurements(
                origImg, imgName, method_name, prediction_method, ratio_of_ones,
                total_embeddings, el_mode, segments, step_size,
                **params  # å±•é–‹æ–¹æ³•ç‰¹å®šåƒæ•¸
            )
            
            # å„²å­˜çµæœ
            all_results[method_name] = results_df
            
            # ä¿å­˜ç‚ºCSV
            results_df.to_csv(f"{comparison_dir}/{method_name}_{predictor}_precise.csv", index=False)
            
            # è¨˜éŒ„å®Œæˆ
            with open(log_file, 'a') as f:
                f.write(f"Completed measurements for {method_name} method\n")
                f.write(f"Results saved to {comparison_dir}/{method_name}_{predictor}_precise.csv\n\n")
            
        except Exception as e:
            print(f"Error processing {method_name}: {str(e)}")
            with open(log_file, 'a') as f:
                f.write(f"Error processing {method_name}: {str(e)}\n")
                f.write(traceback.format_exc())
                f.write("\n\n")
            
            # ç¢ºä¿æ¸…ç†è¨˜æ†¶é«”
            cleanup_memory()
    
    # å¦‚æœæœ‰çµæœï¼Œå‰µå»ºæ¯”è¼ƒåœ–è¡¨
    if all_results:
        plot_method_comparison(all_results, imgName, predictor, comparison_dir)
        create_comparative_table(all_results, f"{comparison_dir}/method_comparison_table.csv")
        print(f"Comparison plots saved to {comparison_dir}")
    
    return all_results

def plot_predictor_comparison(all_results, imgName, method, output_dir):
    """
    ç¹ªè£½å¤šé æ¸¬å™¨ç²¾ç¢ºæ¸¬é‡çµæœçš„æ¯”è¼ƒåœ–è¡¨ï¼Œä¸¦ä¿®å¾© DataFrame è­¦å‘Š
    
    Parameters:
    -----------
    all_results : dict
        åŒ…å«å„é æ¸¬å™¨æ¸¬é‡çµæœçš„å­—å…¸
    imgName : str
        åœ–åƒåç¨±
    method : str
        ä½¿ç”¨çš„æ–¹æ³•
    output_dir : str
        è¼¸å‡ºç›®éŒ„
    """
    # è¨­ç½®ä¸åŒé æ¸¬æ–¹æ³•çš„é¡è‰²å’Œæ¨™è¨˜
    colors = {
        'proposed': 'blue',
        'med': 'red',
        'gap': 'green',
        'rhombus': 'purple'
    }
    
    markers = {
        'proposed': 'o',
        'med': 's',
        'gap': '^',
        'rhombus': 'D'
    }
    
    # å‰µå»ºBPP-PSNRæ¯”è¼ƒåœ–
    plt.figure(figsize=(12, 8))
    
    for predictor, df in all_results.items():
        plt.plot(df['BPP'], df['PSNR'], 
                 color=colors.get(predictor, 'black'),
                 linewidth=2.5,
                 marker=markers.get(predictor, 'x'),
                 markersize=8,
                 label=f'Predictor: {predictor}')
    
    plt.xlabel('Bits Per Pixel (BPP)', fontsize=14)
    plt.ylabel('PSNR (dB)', fontsize=14)
    plt.title(f'Precise Measurement Comparison of Different Predictors\n'
              f'Method: {method}, Image: {imgName}', fontsize=16)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.legend(fontsize=12)
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/comparison_bpp_psnr.png", dpi=300)
    plt.close()  # é—œé–‰åœ–è¡¨
    
    # å‰µå»ºBPP-SSIMæ¯”è¼ƒåœ–
    plt.figure(figsize=(12, 8))
    
    for predictor, df in all_results.items():
        plt.plot(df['BPP'], df['SSIM'], 
                 color=colors.get(predictor, 'black'),
                 linewidth=2.5,
                 marker=markers.get(predictor, 'x'),
                 markersize=8,
                 label=f'Predictor: {predictor}')
    
    plt.xlabel('Bits Per Pixel (BPP)', fontsize=14)
    plt.ylabel('SSIM', fontsize=14)
    plt.title(f'Precise Measurement Comparison of Different Predictors\n'
              f'Method: {method}, Image: {imgName}', fontsize=16)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.legend(fontsize=12)
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/comparison_bpp_ssim.png", dpi=300)
    plt.close()  # é—œé–‰åœ–è¡¨
    
    # å‰µå»ºBPP-Histogram Correlationæ¯”è¼ƒåœ–
    plt.figure(figsize=(12, 8))
    
    for predictor, df in all_results.items():
        plt.plot(df['BPP'], df['Hist_Corr'], 
                 color=colors.get(predictor, 'black'),
                 linewidth=2.5,
                 marker=markers.get(predictor, 'x'),
                 markersize=8,
                 label=f'Predictor: {predictor}')
    
    plt.xlabel('Bits Per Pixel (BPP)', fontsize=14)
    plt.ylabel('Histogram Correlation', fontsize=14)
    plt.title(f'Precise Measurement Comparison of Different Predictors\n'
              f'Method: {method}, Image: {imgName}', fontsize=16)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.legend(fontsize=12)
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/comparison_bpp_hist_corr.png", dpi=300)
    plt.close()  # é—œé–‰åœ–è¡¨
    
    # å‰µå»ºCapacity Percentage-PSNRæ¯”è¼ƒåœ–
    plt.figure(figsize=(12, 8))
    
    for predictor, df in all_results.items():
        plt.plot(df['Target_Percentage'], df['PSNR'], 
                 color=colors.get(predictor, 'black'),
                 linewidth=2.5,
                 marker=markers.get(predictor, 'x'),
                 markersize=8,
                 label=f'Predictor: {predictor}')
    
    plt.xlabel('Capacity Percentage (%)', fontsize=14)
    plt.ylabel('PSNR (dB)', fontsize=14)
    plt.title(f'PSNR vs Capacity Percentage Comparison\n'
              f'Method: {method}, Image: {imgName}', fontsize=16)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.legend(fontsize=12)
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/comparison_percentage_psnr.png", dpi=300)
    plt.close()  # é—œé–‰åœ–è¡¨
    
    # å‰µå»ºCapacity Percentage-SSIMæ¯”è¼ƒåœ–
    plt.figure(figsize=(12, 8))
    
    for predictor, df in all_results.items():
        plt.plot(df['Target_Percentage'], df['SSIM'], 
                 color=colors.get(predictor, 'black'),
                 linewidth=2.5,
                 marker=markers.get(predictor, 'x'),
                 markersize=8,
                 label=f'Predictor: {predictor}')
    
    plt.xlabel('Capacity Percentage (%)', fontsize=14)
    plt.ylabel('SSIM', fontsize=14)
    plt.title(f'SSIM vs Capacity Percentage Comparison\n'
              f'Method: {method}, Image: {imgName}', fontsize=16)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.legend(fontsize=12)
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/comparison_percentage_ssim.png", dpi=300)
    plt.close()  # é—œé–‰åœ–è¡¨
    
    # å‰µå»ºè™•ç†æ™‚é–“æ¯”è¼ƒåœ–
    plt.figure(figsize=(12, 8))
    
    for predictor, df in all_results.items():
        plt.plot(df['Target_Percentage'], df['Processing_Time'], 
                 color=colors.get(predictor, 'black'),
                 linewidth=2.5,
                 marker=markers.get(predictor, 'x'),
                 markersize=8,
                 label=f'Predictor: {predictor}')
    
    plt.xlabel('Capacity Percentage (%)', fontsize=14)
    plt.ylabel('Processing Time (seconds)', fontsize=14)
    plt.title(f'Processing Time Comparison\n'
              f'Method: {method}, Image: {imgName}', fontsize=16)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.legend(fontsize=12)
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/comparison_processing_time.png", dpi=300)
    plt.close()  # é—œé–‰åœ–è¡¨
    
    # å‰µå»ºæœ€å¤§åµŒå…¥å®¹é‡æ¯”è¼ƒåœ– (æ¢å½¢åœ–)
    plt.figure(figsize=(12, 8))
    
    max_payloads = []
    predictor_names = []
    
    for predictor, df in all_results.items():
        max_row = df.loc[df['Target_Percentage'] == 100.0]
        if not max_row.empty:
            # ä¿®æ­£ï¼šä½¿ç”¨ iloc[0] ä¾†å–å¾— Series ä¸­çš„å–®ä¸€å€¼
            max_payloads.append(float(max_row['Actual_Payload'].iloc[0]))
            predictor_names.append(predictor)
    
    bars = plt.bar(predictor_names, max_payloads, color=[colors.get(p, 'gray') for p in predictor_names])
    
    # åœ¨æ¢å½¢ä¸Šæ·»åŠ æ•¸å€¼æ¨™ç±¤
    for bar, payload in zip(bars, max_payloads):
        plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.1,
                f'{int(payload)}',
                ha='center', va='bottom', fontsize=12)
    
    plt.xlabel('Predictor', fontsize=14)
    plt.ylabel('Maximum Payload (bits)', fontsize=14)
    plt.title(f'Maximum Payload Comparison\n'
              f'Method: {method}, Image: {imgName}', fontsize=16)
    plt.grid(True, linestyle='--', alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/comparison_max_payload.png", dpi=300)
    plt.close()  # é—œé–‰åœ–è¡¨

def plot_method_comparison(all_results, imgName, predictor, output_dir):
    """
    ç‚ºä½¿ç”¨ç›¸åŒé æ¸¬å™¨çš„ä¸åŒæ–¹æ³•å‰µå»ºæ¯”è¼ƒåœ–è¡¨
    
    Parameters:
    -----------
    all_results : dict
        åŒ…å«æ¯å€‹æ–¹æ³•çµæœçš„å­—å…¸ {æ–¹æ³•åç¨±: DataFrame}
    imgName : str
        åœ–åƒåç¨±
    predictor : str
        æ‰€æœ‰æ–¹æ³•ä½¿ç”¨çš„é æ¸¬å™¨
    output_dir : str
        è¼¸å‡ºç›®éŒ„
    """
    
    # ç‚ºä¸åŒæ–¹æ³•è¨­ç½®é¡è‰²å’Œæ¨™è¨˜
    colors = {
        'rotation': 'blue',
        'split': 'green',
        'quadtree': 'red',
        'custom': 'purple'  # ç”¨æ–¼å…¶ä»–æ–¹æ³•
    }
    
    markers = {
        'rotation': 'o',
        'split': 's',
        'quadtree': '^',
        'custom': 'D'  # ç”¨æ–¼å…¶ä»–æ–¹æ³•
    }
    
    # å‰µå»º BPP-PSNR æ¯”è¼ƒåœ–
    plt.figure(figsize=(12, 8))
    
    for method, df in all_results.items():
        color = colors.get(method, 'black')
        marker = markers.get(method, 'x')
        
        plt.plot(df['BPP'], df['PSNR'], 
                 color=color,
                 linewidth=2.5,
                 marker=marker,
                 markersize=8,
                 label=f'Method: {method}')
    
    plt.xlabel('Bits Per Pixel (BPP)', fontsize=14)
    plt.ylabel('PSNR (dB)', fontsize=14)
    plt.title(f'Method Comparison with {predictor.capitalize()} Predictor\n'
              f'Image: {imgName}', fontsize=16)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.legend(fontsize=12)
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/method_comparison_bpp_psnr.png", dpi=300)
    plt.close()
    
    # å‰µå»º BPP-SSIM æ¯”è¼ƒåœ–
    plt.figure(figsize=(12, 8))
    
    for method, df in all_results.items():
        color = colors.get(method, 'black')
        marker = markers.get(method, 'x')
        
        plt.plot(df['BPP'], df['SSIM'], 
                 color=color,
                 linewidth=2.5,
                 marker=marker,
                 markersize=8,
                 label=f'Method: {method}')
    
    plt.xlabel('Bits Per Pixel (BPP)', fontsize=14)
    plt.ylabel('SSIM', fontsize=14)
    plt.title(f'Method Comparison with {predictor.capitalize()} Predictor\n'
              f'Image: {imgName}', fontsize=16)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.legend(fontsize=12)
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/method_comparison_bpp_ssim.png", dpi=300)
    plt.close()
    
    # å¦‚æœæœ‰ç›´æ–¹åœ–ç›¸é—œæ€§æ•¸æ“šï¼Œå‰µå»º BPP-Hist_Corr æ¯”è¼ƒåœ–
    if 'Hist_Corr' in next(iter(all_results.values())):
        plt.figure(figsize=(12, 8))
        
        for method, df in all_results.items():
            color = colors.get(method, 'black')
            marker = markers.get(method, 'x')
            
            plt.plot(df['BPP'], df['Hist_Corr'], 
                    color=color,
                    linewidth=2.5,
                    marker=marker,
                    markersize=8,
                    label=f'Method: {method}')
        
        plt.xlabel('Bits Per Pixel (BPP)', fontsize=14)
        plt.ylabel('Histogram Correlation', fontsize=14)
        plt.title(f'Histogram Correlation Comparison with {predictor.capitalize()} Predictor\n'
                f'Image: {imgName}', fontsize=16)
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.legend(fontsize=12)
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/method_comparison_bpp_histcorr.png", dpi=300)
        plt.close()
    
    # å‰µå»º PSNR-Payload æ¯”è¼ƒåœ–ï¼ˆé¡å¤–çš„è¦–è§’ï¼‰
    plt.figure(figsize=(12, 8))
    
    for method, df in all_results.items():
        color = colors.get(method, 'black')
        marker = markers.get(method, 'x')
        
        plt.plot(df['Actual_Payload'], df['PSNR'], 
                 color=color,
                 linewidth=2.5,
                 marker=marker,
                 markersize=8,
                 label=f'Method: {method}')
    
    plt.xlabel('Payload (bits)', fontsize=14)
    plt.ylabel('PSNR (dB)', fontsize=14)
    plt.title(f'Payload-PSNR Comparison with {predictor.capitalize()} Predictor\n'
              f'Image: {imgName}', fontsize=16)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.legend(fontsize=12)
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/method_comparison_payload_psnr.png", dpi=300)
    plt.close()
    
    # å‰µå»ºç¶œåˆé›·é”åœ–ï¼ˆåƒ…åœ¨æœ‰å¤šå€‹æ–¹æ³•æ™‚æ‰æœ‰æ„ç¾©ï¼‰
    if len(all_results) >= 2:
        create_radar_chart(all_results, predictor, imgName, output_dir)

def create_comparative_table(all_results, output_path):
    """
    å‰µå»ºæ‰€æœ‰æ–¹æ³•åœ¨ç›¸ä¼¼ BPP å€¼ä¸‹çš„æ¯”è¼ƒè¡¨
    
    Parameters:
    -----------
    all_results : dict
        åŒ…å«æ¯å€‹æ–¹æ³•çµæœçš„å­—å…¸ {æ–¹æ³•åç¨±: DataFrame}
    output_path : str
        ä¿å­˜æ¯”è¼ƒè¡¨çš„è·¯å¾‘
    
    Returns:
    --------
    pandas.DataFrame
        æ¯”è¼ƒè¡¨
    """
    
    # é¦–å…ˆï¼Œè­˜åˆ¥å…±åŒçš„ BPP ç¯„åœ
    all_bpp_values = []
    for method, df in all_results.items():
        all_bpp_values.extend(df['BPP'].tolist())
    
    # æ’åºä¸¦å»é™¤é‡è¤‡
    all_bpp_values = sorted(list(set([round(bpp, 4) for bpp in all_bpp_values])))
    
    # å‰µå»ºå›ºå®šé–“éš”çš„åƒè€ƒ BPP å€¼
    min_bpp = min(all_bpp_values)
    max_bpp = max(all_bpp_values)
    step = (max_bpp - min_bpp) / 20  # 20 å€‹åƒè€ƒé»
    reference_bpps = [round(min_bpp + i * step, 4) for i in range(21)]  # åŒ…æ‹¬çµ‚é»
    
    # å‰µå»ºå…·æœ‰å…±åŒ BPP å€¼çš„ DataFrame
    comp_data = {'BPP': reference_bpps}
    
    # ç‚ºæ¯å€‹æ–¹æ³•æ‰¾åˆ°æœ€æ¥è¿‘çš„ PSNR å’Œ SSIM å€¼
    for method, df in all_results.items():
        psnr_values = []
        ssim_values = []
        hist_corr_values = []
        
        for bpp in reference_bpps:
            # åœ¨æ­¤æ–¹æ³•çš„çµæœä¸­æ‰¾åˆ°æœ€æ¥è¿‘çš„ BPP
            closest_idx = (df['BPP'] - bpp).abs().idxmin()
            psnr_values.append(df.loc[closest_idx, 'PSNR'])
            ssim_values.append(df.loc[closest_idx, 'SSIM'])
            if 'Hist_Corr' in df.columns:
                hist_corr_values.append(df.loc[closest_idx, 'Hist_Corr'])
        
        comp_data[f'{method}_PSNR'] = psnr_values
        comp_data[f'{method}_SSIM'] = ssim_values
        if hist_corr_values:
            comp_data[f'{method}_Hist_Corr'] = hist_corr_values
    
    # æ·»åŠ æ–¹æ³•ä¹‹é–“çš„æ€§èƒ½å·®ç•°
    if len(all_results) > 1:
        methods = list(all_results.keys())
        
        # ç²å–ç¬¬ä¸€å€‹æ–¹æ³•ä½œç‚ºåŸºæº–
        base_method = methods[0]
        
        # è¨ˆç®—å…¶ä»–æ–¹æ³•èˆ‡åŸºæº–æ–¹æ³•ä¹‹é–“çš„ PSNR å·®ç•°
        for method in methods[1:]:
            psnr_diff = [comp_data[f'{method}_PSNR'][i] - comp_data[f'{base_method}_PSNR'][i] 
                         for i in range(len(reference_bpps))]
            comp_data[f'{method}_vs_{base_method}_PSNR_diff'] = psnr_diff
    
    # å‰µå»ºä¸¦ä¿å­˜ DataFrame
    comp_df = pd.DataFrame(comp_data)
    comp_df.to_csv(output_path, index=False)
    
    # å‰µå»º LaTeX æ ¼å¼è¡¨æ ¼
    latex_path = output_path.replace('.csv', '.tex')
    try:
        with open(latex_path, 'w') as f:
            # åªé¸æ“‡éƒ¨åˆ†è¡Œå’Œåˆ—ä»¥ç²å¾—ç°¡æ½”çš„ LaTeX è¡¨æ ¼
            # å– 5 å€‹å‡å‹»åˆ†ä½ˆçš„é»
            indices = [0, 5, 10, 15, 20]  # ä½æ–¼ 0%, 25%, 50%, 75%, 100%
            
            # ç‚ºæ¯å€‹æ–¹æ³•é¸æ“‡ PSNR åˆ—
            columns = ['BPP'] + [f'{method}_PSNR' for method in all_results.keys()]
            
            # å‰µå»ºå­è¡¨
            sub_df = comp_df.iloc[indices][columns]
            
            # é‡å‘½ååˆ—ä»¥ä¾¿æ›´å¥½åœ°é¡¯ç¤º
            column_mapping = {f'{method}_PSNR': method.capitalize() for method in all_results.keys()}
            column_mapping['BPP'] = 'BPP'
            sub_df = sub_df.rename(columns=column_mapping)
            
            # ç”Ÿæˆ LaTeX è¡¨æ ¼
            latex_table = sub_df.to_latex(index=False, float_format="%.2f")
            
            # æ·»åŠ è¡¨æ ¼æ¨™é¡Œå’Œæ¨™ç±¤
            latex_header = "\\begin{table}[h]\n\\centering\n\\caption{PSNR Comparison at Different BPP Levels}\n\\label{tab:psnr_comparison}\n"
            latex_footer = "\\end{table}"
            
            # å¯«å…¥å®Œæ•´è¡¨æ ¼
            f.write(latex_header + latex_table + latex_footer)
    except Exception as e:
        print(f"Could not generate LaTeX table: {e}")
    
    return comp_df

def create_radar_chart(all_results, predictor, imgName, output_dir):
    """
    å‰µå»ºæ¯”è¼ƒä¸åŒæ–¹æ³•æ€§èƒ½çš„é›·é”åœ–
    
    Parameters:
    -----------
    all_results : dict
        åŒ…å«æ¯å€‹æ–¹æ³•çµæœçš„å­—å…¸ {æ–¹æ³•åç¨±: DataFrame}
    predictor : str
        æ‰€æœ‰æ–¹æ³•ä½¿ç”¨çš„é æ¸¬å™¨
    imgName : str
        åœ–åƒåç¨±
    output_dir : str
        è¼¸å‡ºç›®éŒ„
    """
    
    # æª¢æŸ¥æ˜¯å¦æœ‰è¶³å¤ çš„æ–¹æ³•ä¾†å‰µå»ºé›·é”åœ–
    if len(all_results) < 2:
        return
    
    # é¸æ“‡ç”¨æ–¼æ¯”è¼ƒçš„ BPP æ°´å¹³
    # æ‰¾åˆ°æ‰€æœ‰æ–¹æ³•å…±æœ‰çš„ BPP ç¯„åœ
    all_bpp_min = max([df['BPP'].min() for df in all_results.values()])
    all_bpp_max = min([df['BPP'].max() for df in all_results.values()])
    
    # å¦‚æœå…±åŒç¯„åœç„¡æ•ˆï¼Œå‰‡é€€å‡º
    if all_bpp_min >= all_bpp_max:
        print("Cannot create radar chart: methods have non-overlapping BPP ranges")
        return
    
    # é¸æ“‡ 3 å€‹ BPP é»é€²è¡Œæ¯”è¼ƒï¼šä½ã€ä¸­ã€é«˜
    bpp_levels = [
        all_bpp_min + (all_bpp_max - all_bpp_min) * 0.25,  # ä½ BPP
        all_bpp_min + (all_bpp_max - all_bpp_min) * 0.5,   # ä¸­ BPP
        all_bpp_min + (all_bpp_max - all_bpp_min) * 0.75   # é«˜ BPP
    ]
    
    # é›·é”åœ–é¡åˆ¥
    categories = ['PSNR', 'SSIM', 'Speed']
    
    # ç‚ºä¸åŒæ–¹æ³•è¨­ç½®é¡è‰²
    colors = {
        'rotation': 'blue',
        'split': 'green',
        'quadtree': 'red',
        'custom': 'purple'  # ç”¨æ–¼å…¶ä»–æ–¹æ³•
    }
    
    # ç”¨æ–¼é¡¯ç¤ºæ¯å€‹æ–¹æ³•åœ¨å„å€‹é¡åˆ¥çš„æ€§èƒ½
    fig, axs = plt.subplots(1, 3, figsize=(18, 6), subplot_kw=dict(polar=True))
    
    # ç‚ºæ¯å€‹ BPP æ°´å¹³å‰µå»ºé›·é”åœ–
    for i, bpp in enumerate(bpp_levels):
        ax = axs[i]
        
        # ç²å–æ¯å€‹æ–¹æ³•åœ¨æ­¤ BPP ä¸‹çš„æ€§èƒ½
        performances = {}
        for method, df in all_results.items():
            # æ‰¾åˆ°æœ€æ¥è¿‘çš„ BPP
            closest_idx = (df['BPP'] - bpp).abs().idxmin()
            closest_row = df.loc[closest_idx]
            
            # è¨˜éŒ„æ€§èƒ½æŒ‡æ¨™
            performances[method] = {
                'PSNR': closest_row['PSNR'],
                'SSIM': closest_row['SSIM'],
                'Speed': 1.0 / closest_row['Processing_Time']  # é€Ÿåº¦æ˜¯è™•ç†æ™‚é–“çš„å€’æ•¸
            }
        
        # æ­£è¦åŒ–æ€§èƒ½æŒ‡æ¨™åˆ° 0-1 ç¯„åœ
        normalized_performances = {}
        for category in categories:
            values = [performances[method][category] for method in performances]
            min_val = min(values)
            max_val = max(values)
            
            if max_val > min_val:
                for method in performances:
                    if category not in normalized_performances:
                        normalized_performances[category] = {}
                    
                    # æ­£è¦åŒ–ï¼Œç¢ºä¿è¼ƒé«˜çš„å€¼ç¸½æ˜¯æ›´å¥½
                    normalized_performances[category][method] = (performances[method][category] - min_val) / (max_val - min_val)
            else:
                # å¦‚æœæ‰€æœ‰å€¼ç›¸åŒï¼Œå‰‡è¨­ç‚º 1
                for method in performances:
                    if category not in normalized_performances:
                        normalized_performances[category] = {}
                    normalized_performances[category][method] = 1.0
        
        # è¨­ç½®è§’åº¦
        angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False).tolist()
        angles += angles[:1]  # é–‰åˆåœ–å½¢
        
        # è¨­ç½®é›·é”åœ–
        ax.set_theta_offset(np.pi / 2)
        ax.set_theta_direction(-1)
        ax.set_rlabel_position(0)
        
        # æ·»åŠ é¡åˆ¥æ¨™ç±¤
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(categories)
        
        # è¨­ç½® y è»¸åˆ»åº¦
        ax.set_yticks([0.25, 0.5, 0.75, 1.0])
        ax.set_yticklabels(['0.25', '0.5', '0.75', '1.0'])
        ax.set_rlim(0, 1)
        
        # ç¹ªè£½æ¯å€‹æ–¹æ³•çš„é›·é”åœ–
        for method in performances:
            color = colors.get(method, 'black')
            
            # ç²å–æ€§èƒ½å€¼
            values = [normalized_performances[cat][method] for cat in categories]
            values += values[:1]  # é–‰åˆåœ–å½¢
            
            # ç¹ªè£½é›·é”ç·š
            ax.plot(angles, values, color=color, linewidth=2, label=method)
            ax.fill(angles, values, color=color, alpha=0.1)
        
        # è¨­ç½®æ¨™é¡Œ
        ax.set_title(f'BPP = {bpp:.4f}', size=14, y=1.1)
        
        # åªåœ¨ç¬¬ä¸€å€‹å­åœ–é¡¯ç¤ºåœ–ä¾‹
        if i == 0:
            ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))
    
    plt.suptitle(f'Performance Comparison of Different Methods with {predictor.capitalize()} Predictor', size=16)
    plt.tight_layout()
    
    # ä¿å­˜åœ–è¡¨
    plt.savefig(f"{output_dir}/method_radar_comparison.png", dpi=300, bbox_inches='tight')
    plt.close()

def create_wide_format_tables(all_results, output_dir):
    """
    å‰µå»ºå¯¬æ ¼å¼è¡¨æ ¼ï¼Œä¾¿æ–¼è«–æ–‡ä½¿ç”¨
    
    Parameters:
    -----------
    all_results : dict
        åŒ…å«å„é æ¸¬å™¨æ¸¬é‡çµæœçš„å­—å…¸
    output_dir : str
        è¼¸å‡ºç›®éŒ„
    """
    # å‰µå»ºPSNRè¡¨æ ¼ (åˆ—ï¼šç™¾åˆ†æ¯”ï¼Œåˆ—ï¼šé æ¸¬å™¨)
    psnr_table = {'Percentage': []}
    ssim_table = {'Percentage': []}
    hist_corr_table = {'Percentage': []}
    
    # ç¢ºå®šæ‰€æœ‰ç™¾åˆ†æ¯”å€¼
    percentages = []
    for df in all_results.values():
        percentages.extend(df['Target_Percentage'].tolist())
    
    # å»é‡ä¸¦æ’åº
    percentages = sorted(list(set(percentages)))
    psnr_table['Percentage'] = percentages
    ssim_table['Percentage'] = percentages
    hist_corr_table['Percentage'] = percentages
    
    # å¡«å……å„é æ¸¬å™¨çš„æ•¸æ“š
    for predictor, df in all_results.items():
        psnr_values = []
        ssim_values = []
        hist_corr_values = []
        
        for percentage in percentages:
            # æ‰¾åˆ°æœ€æ¥è¿‘çš„ç™¾åˆ†æ¯”è¡Œ
            closest_idx = (df['Target_Percentage'] - percentage).abs().idxmin()
            row = df.loc[closest_idx]
            
            psnr_values.append(row['PSNR'])
            ssim_values.append(row['SSIM'])
            hist_corr_values.append(row['Hist_Corr'])
        
        psnr_table[predictor] = psnr_values
        ssim_table[predictor] = ssim_values
        hist_corr_table[predictor] = hist_corr_values
    
    # å‰µå»ºDataFrame
    psnr_df = pd.DataFrame(psnr_table)
    ssim_df = pd.DataFrame(ssim_table)
    hist_corr_df = pd.DataFrame(hist_corr_table)
    
    # ä¿å­˜è¡¨æ ¼
    psnr_df.to_csv(f"{output_dir}/wide_format_psnr.csv", index=False)
    ssim_df.to_csv(f"{output_dir}/wide_format_ssim.csv", index=False)
    hist_corr_df.to_csv(f"{output_dir}/wide_format_hist_corr.csv", index=False)
    
    # å‰µå»ºLaTeXæ ¼å¼è¡¨æ ¼
    with open(f"{output_dir}/latex_table_psnr.txt", 'w') as f:
        f.write(psnr_df.to_latex(index=False, float_format="%.2f"))
    
    with open(f"{output_dir}/latex_table_ssim.txt", 'w') as f:
        f.write(ssim_df.to_latex(index=False, float_format="%.4f"))
    
    with open(f"{output_dir}/latex_table_hist_corr.txt", 'w') as f:
        f.write(hist_corr_df.to_latex(index=False, float_format="%.4f"))
    
    print(f"Wide format tables saved to {output_dir}")

# =============================================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šèˆŠç‰ˆæ¸¬é‡å’Œç¹ªåœ–å‡½æ•¸ï¼ˆä¿ç•™ä»¥ä¿æŒå‘å¾Œå…¼å®¹æ€§ï¼‰
# =============================================================================

def generate_interval_statistics(original_img, stages, total_payload, segments=15):
    """
    æ ¹æ“šç¸½åµŒå…¥å®¹é‡ç”Ÿæˆå‡å‹»åˆ†å¸ƒçš„çµ±è¨ˆæ•¸æ“šè¡¨æ ¼
    (è¿‘ä¼¼æ–¹æ³•ï¼Œä½¿ç”¨å·²æœ‰çš„éšæ®µçµæœé€²è¡Œæ’å€¼)
    
    Parameters:
    -----------
    original_img : numpy.ndarray
        åŸå§‹åœ–åƒ
    stages : list
        åŒ…å«åµŒå…¥å„éšæ®µè³‡è¨Šçš„åˆ—è¡¨
    total_payload : int
        ç¸½åµŒå…¥å®¹é‡
    segments : int
        è¦ç”Ÿæˆçš„æ•¸æ“šé»æ•¸é‡ï¼Œé»˜èªç‚º15
        
    Returns:
    --------
    tuple
        (DataFrame, PrettyTable) åŒ…å«çµ±è¨ˆæ•¸æ“šçš„DataFrameå’Œæ ¼å¼åŒ–è¡¨æ ¼
    """
    # ç¢ºä¿è¼¸å…¥æ•¸æ“šé¡å‹æ­£ç¢º
    if isinstance(original_img, cp.ndarray):
        original_img = cp.asnumpy(original_img)
    
    # ç¸½åƒç´ æ•¸ç”¨æ–¼è¨ˆç®—BPP
    total_pixels = original_img.size
    
    # è¨ˆç®—é–“éš”å’Œæ•¸æ“šé»
    if total_payload <= 0:
        print("Warning: Total payload is zero or negative. No statistics generated.")
        return None, None
        
    # ç¢ºä¿è‡³å°‘æœ‰2å€‹æ®µ
    segments = max(2, min(segments, total_payload))
    
    # è¨ˆç®—æ¯å€‹ç´šè·çš„ç›®æ¨™åµŒå…¥é‡
    payload_interval = total_payload / segments
    payload_points = [int(i * payload_interval) for i in range(1, segments + 1)]
    
    # æœ€å¾Œä¸€å€‹é»ç¢ºä¿æ˜¯ç¸½åµŒå…¥é‡
    payload_points[-1] = total_payload
    
    # åˆå§‹åŒ–çµæœæ•¸æ“š
    results = []
    
    # ç´¯è¨ˆå„éšæ®µçš„åµŒå…¥é‡
    accumulated_payload = 0
    current_stage_index = 0
    current_stage_img = None
    
    for target_payload in payload_points:
        # æ¨¡æ“¬åµŒå…¥åˆ°ç›®æ¨™åµŒå…¥é‡çš„åœ–åƒç‹€æ…‹
        while accumulated_payload < target_payload and current_stage_index < len(stages):
            current_stage = stages[current_stage_index]
            stage_payload = current_stage['payload']
            current_stage_img = cp.asnumpy(current_stage['stage_img'])
            
            if accumulated_payload + stage_payload <= target_payload:
                # å®Œæ•´åŒ…å«ç•¶å‰éšæ®µ
                accumulated_payload += stage_payload
                current_stage_index += 1
            else:
                # éƒ¨åˆ†åŒ…å«ç•¶å‰éšæ®µ - éœ€è¦é€²è¡Œæ’å€¼
                # æ³¨æ„ï¼šé€™è£¡ä½¿ç”¨ç·šæ€§æ’å€¼ä¾†ä¼°è¨ˆPSNRå’ŒSSIMï¼Œå¯¦éš›ä¸Šå¯èƒ½éœ€è¦æ›´ç²¾ç¢ºçš„æ¨¡æ“¬
                break
        
        # ç¢ºä¿current_stage_imgä¸ç‚ºNone
        if current_stage_img is None and current_stage_index > 0:
            current_stage_img = cp.asnumpy(stages[current_stage_index-1]['stage_img'])
        elif current_stage_img is None:
            print("Warning: No valid stage image found.")
            continue
            
        # è¨ˆç®—æ€§èƒ½æŒ‡æ¨™
        psnr = calculate_psnr(original_img, current_stage_img)
        ssim = calculate_ssim(original_img, current_stage_img)
        hist_corr = histogram_correlation(
            np.histogram(original_img, bins=256, range=(0, 255))[0],
            np.histogram(current_stage_img, bins=256, range=(0, 255))[0]
        )
        bpp = target_payload / total_pixels
        
        # æ·»åŠ åˆ°çµæœåˆ—è¡¨
        results.append({
            'Payload': target_payload,
            'BPP': bpp,
            'PSNR': psnr,
            'SSIM': ssim,
            'Hist_Corr': hist_corr
        })
    
    # å‰µå»ºDataFrame
    df = pd.DataFrame(results)
    
    # å‰µå»ºPrettyTable
    table = PrettyTable()
    table.field_names = ["Payload", "BPP", "PSNR", "SSIM", "Hist_Corr"]
    
    for result in results:
        table.add_row([
            result['Payload'],
            f"{result['BPP']:.6f}",
            f"{result['PSNR']:.2f}",
            f"{result['SSIM']:.4f}",
            f"{result['Hist_Corr']:.4f}"
        ])
    
    return df, table

def save_interval_statistics(df, imgName, method, prediction_method, base_dir="./Prediction_Error_Embedding/outcome/plots"):
    """
    ä¿å­˜çµ±è¨ˆæ•¸æ“šåˆ°CSVå’ŒNPYæ–‡ä»¶
    
    Parameters:
    -----------
    df : pandas.DataFrame
        åŒ…å«çµ±è¨ˆæ•¸æ“šçš„DataFrame
    imgName : str
        åœ–åƒåç¨±
    method : str
        ä½¿ç”¨çš„æ–¹æ³•
    prediction_method : str
        ä½¿ç”¨çš„é æ¸¬æ–¹æ³•
    base_dir : str, optional
        åŸºæœ¬å„²å­˜ç›®éŒ„ï¼Œé»˜èªç‚º "./Prediction_Error_Embedding/outcome/plots"
    """
    # ç¢ºä¿ç›®éŒ„å­˜åœ¨
    os.makedirs(f"{base_dir}/{imgName}", exist_ok=True)
    
    # ä¿å­˜CSV
    csv_path = f"{base_dir}/{imgName}/interval_stats_{method}_{prediction_method}.csv"
    df.to_csv(csv_path, index=False)
    print(f"Interval statistics saved to {csv_path}")
    
    # ä¿å­˜NPY
    npy_path = f"{base_dir}/{imgName}/interval_stats_{method}_{prediction_method}.npy"
    np.save(npy_path, df.to_dict('records'))
    print(f"Interval statistics saved to {npy_path}")
    
    return csv_path, npy_path

def run_multiple_predictors(imgName, filetype="png", method="quadtree", 
                           predictor_ratios=None, total_embeddings=5, 
                           el_mode=0, use_different_weights=False,
                           split_size=2, block_base=False, 
                           quad_tree_params=None, stats_segments=15):
    """
    è‡ªå‹•é‹è¡Œå¤šç¨®é æ¸¬æ–¹æ³•ä¸¦ç”Ÿæˆæ¯”è¼ƒçµæœ
    (ä½¿ç”¨è¿‘ä¼¼æ–¹æ³•)
    
    Parameters:
    -----------
    imgName : str
        åœ–åƒåç¨±
    filetype : str
        åœ–åƒæª”æ¡ˆé¡å‹
    method : str
        ä½¿ç”¨çš„æ–¹æ³•
    predictor_ratios : dict
        å„é æ¸¬å™¨çš„ratio_of_onesè¨­ç½®
    total_embeddings : int
        ç¸½åµŒå…¥æ¬¡æ•¸
    el_mode : int
        ELæ¨¡å¼
    use_different_weights : bool
        æ˜¯å¦å°æ¯å€‹å­åœ–åƒä½¿ç”¨ä¸åŒçš„æ¬Šé‡
    split_size : int
        åˆ†å‰²å¤§å°
    block_base : bool
        æ˜¯å¦ä½¿ç”¨block baseæ–¹å¼
    quad_tree_params : dict
        å››å‰æ¨¹åƒæ•¸
    stats_segments : int
        çµ±è¨ˆåˆ†æ®µæ•¸é‡
        
    Returns:
    --------
    tuple
        (results_df, all_stats) åŒ…å«æ¯”è¼ƒçµæœçš„DataFrameå’Œçµ±è¨ˆæ•¸æ“š
    """
    
    from embedding import (
        pee_process_with_rotation_cuda,
        pee_process_with_split_cuda
    )
    from quadtree import pee_process_with_quadtree_cuda
    
    # è¨­ç½®é»˜èªçš„é æ¸¬å™¨ratioå­—å…¸
    if predictor_ratios is None:
        predictor_ratios = {
            "PROPOSED": 0.5,
            "MED": 1.0,
            "GAP": 1.0,
            "RHOMBUS": 1.0
        }
    
    # å‰µå»ºå¿…è¦çš„ç›®éŒ„
    comparison_dir = f"./Prediction_Error_Embedding/outcome/plots/{imgName}/comparison"
    os.makedirs(comparison_dir, exist_ok=True)
    
    # é æ¸¬æ–¹æ³•åˆ—è¡¨
    prediction_methods = [
        PredictionMethod.PROPOSED,
        PredictionMethod.MED,
        PredictionMethod.GAP,
        PredictionMethod.RHOMBUS
    ]
    
    # å„²å­˜æ‰€æœ‰é æ¸¬æ–¹æ³•çš„çµ±è¨ˆæ•¸æ“š
    all_stats = {}
    all_results = {}
    
    # è¨˜éŒ„é–‹å§‹æ™‚é–“
    start_time = time.time()
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # å‰µå»ºè¨˜éŒ„æª”æ¡ˆ
    log_file = f"{comparison_dir}/multi_predictor_run_{timestamp}.log"
    with open(log_file, 'w') as f:
        f.write(f"Multi-predictor comparison run started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Image: {imgName}.{filetype}\n")
        f.write(f"Method: {method}\n")
        f.write(f"Parameters: embeddings={total_embeddings}, el_mode={el_mode}\n")
        f.write("Predictor ratio settings:\n")
        for pred, ratio in predictor_ratios.items():
            f.write(f"  {pred}: {ratio}\n")
            
        if method == "quadtree":
            f.write(f"Quadtree params: min_block_size={quad_tree_params['min_block_size']}, variance_threshold={quad_tree_params['variance_threshold']}\n")
        else:
            f.write(f"Split size: {split_size}x{split_size}, block_base={block_base}\n")
        f.write("\n" + "="*80 + "\n\n")
    
    # è®€å–åŸå§‹åœ–åƒ - åªéœ€è¦è®€å–ä¸€æ¬¡
    origImg = cv2.imread(f"./Prediction_Error_Embedding/image/{imgName}.{filetype}", cv2.IMREAD_GRAYSCALE)
    if origImg is None:
        raise ValueError(f"Failed to read image: ./Prediction_Error_Embedding/image/{imgName}.{filetype}")
    origImg = np.array(origImg).astype(np.uint8)
    total_pixels = origImg.size
    
    # ä¾æ¬¡é‹è¡Œæ¯ç¨®é æ¸¬æ–¹æ³•
    for prediction_method in prediction_methods:
        method_name = prediction_method.value.upper()
        
        print(f"\n{'='*80}")
        print(f"Running with {method_name.lower()} predictor...")
        print(f"{'='*80}\n")
        
        # è¨˜éŒ„åˆ°æ—¥èªŒ
        with open(log_file, 'a') as f:
            f.write(f"Starting run with {method_name.lower()} predictor at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # é‡ç½® GPU è¨˜æ†¶é«”
        cp.get_default_memory_pool().free_all_blocks()
        
        # ç²å–ç•¶å‰é æ¸¬å™¨çš„ratio_of_ones
        current_ratio_of_ones = predictor_ratios.get(method_name, 0.5)
        print(f"Using ratio_of_ones = {current_ratio_of_ones}")
        
        # é‡å° MEDã€GAP å’Œ RHOMBUSï¼Œå¼·åˆ¶è¨­ç½® use_different_weights = False
        current_use_weights = use_different_weights
        if prediction_method in [PredictionMethod.MED, PredictionMethod.GAP, PredictionMethod.RHOMBUS]:
            current_use_weights = False
            print(f"Note: Weight optimization disabled for {method_name.lower()} prediction method")
        
        try:
            # åŸ·è¡Œé¸å®šçš„æ–¹æ³•
            if method == "rotation":
                final_pee_img, total_payload, pee_stages = pee_process_with_rotation_cuda(
                    origImg,
                    total_embeddings,
                    current_ratio_of_ones,
                    current_use_weights,
                    split_size,
                    el_mode,
                    prediction_method=prediction_method,
                    target_payload_size=-1  # ä½¿ç”¨æœ€å¤§åµŒå…¥é‡
                )
            elif method == "split":
                final_pee_img, total_payload, pee_stages = pee_process_with_split_cuda(
                    origImg,
                    total_embeddings,
                    current_ratio_of_ones,
                    current_use_weights,
                    split_size,
                    el_mode,
                    block_base,
                    prediction_method=prediction_method,
                    target_payload_size=-1  # ä½¿ç”¨æœ€å¤§åµŒå…¥é‡
                )
            elif method == "quadtree":
                final_pee_img, total_payload, pee_stages = pee_process_with_quadtree_cuda(
                    origImg,
                    total_embeddings,
                    current_ratio_of_ones,
                    current_use_weights,
                    quad_tree_params['min_block_size'],
                    quad_tree_params['variance_threshold'],
                    el_mode,
                    rotation_mode='random',
                    prediction_method=prediction_method,
                    target_payload_size=-1  # ä½¿ç”¨æœ€å¤§åµŒå…¥é‡
                )
            
            # ç”Ÿæˆçµ±è¨ˆæ•¸æ“š
            print("\nGenerating interval statistics...")
            stats_df, stats_table = generate_interval_statistics(
                origImg, pee_stages, total_payload, segments=stats_segments
            )
            
            # ä¿å­˜çµ±è¨ˆæ•¸æ“š
            if stats_df is not None:
                # è¨˜éŒ„çµ±è¨ˆæ•¸æ“š
                stats_df['Predictor'] = method_name.lower()
                stats_df['Ratio_of_Ones'] = current_ratio_of_ones
                all_stats[method_name.lower()] = stats_df
                
                # ä¿å­˜ç‚ºCSV
                csv_path = f"{comparison_dir}/{method_name.lower()}_stats.csv"
                stats_df.to_csv(csv_path, index=False)
                print(f"Statistics saved to {csv_path}")
                
                # å°‡çµæœæ·»åŠ åˆ°å­—å…¸ä¸­
                final_psnr = calculate_psnr(origImg, final_pee_img)
                final_ssim = calculate_ssim(origImg, final_pee_img)
                hist_orig = generate_histogram(origImg)
                hist_final = generate_histogram(final_pee_img)
                final_hist_corr = histogram_correlation(hist_orig, hist_final)
                
                all_results[method_name.lower()] = {
                    'predictor': method_name.lower(),
                    'ratio_of_ones': current_ratio_of_ones,
                    'total_payload': total_payload,
                    'bpp': total_payload / total_pixels,
                    'psnr': final_psnr,
                    'ssim': final_ssim,
                    'hist_corr': final_hist_corr
                }
                
                # è¨˜éŒ„åˆ°æ—¥èªŒ
                with open(log_file, 'a') as f:
                    f.write(f"Run completed for {method_name.lower()} predictor\n")
                    f.write(f"Total payload: {total_payload}\n")
                    f.write(f"PSNR: {final_psnr:.2f}\n")
                    f.write(f"SSIM: {final_ssim:.4f}\n")
                    f.write(f"Processing time: {time.time() - start_time:.2f} seconds\n")
                    f.write("\n" + "-"*60 + "\n\n")
                
                # ä¿å­˜è™•ç†å¾Œçš„åœ–åƒ
                save_image(final_pee_img, 
                          f"{comparison_dir}/{method_name.lower()}_final.png")
            
            else:
                print(f"Warning: No statistics generated for {method_name.lower()}")
                with open(log_file, 'a') as f:
                    f.write(f"Warning: No statistics generated for {method_name.lower()}\n\n")
        
        except Exception as e:
            print(f"Error processing {method_name.lower()}: {str(e)}")
            with open(log_file, 'a') as f:
                f.write(f"Error processing {method_name.lower()}: {str(e)}\n")
                f.write(traceback.format_exc())
                f.write("\n\n")
    
    # æ‰€æœ‰é æ¸¬æ–¹æ³•è™•ç†å®Œæˆå¾Œï¼Œå‰µå»ºæ¯”è¼ƒçµæœ
    if all_stats:
        try:
            # å‰µå»ºçµæœæ‘˜è¦è¡¨
            results_df = pd.DataFrame(list(all_results.values()))
            results_df = results_df[['predictor', 'ratio_of_ones', 'total_payload', 'bpp', 'psnr', 'ssim', 'hist_corr']]
            
            # ä¿å­˜çµæœæ‘˜è¦
            results_csv = f"{comparison_dir}/summary_results.csv"
            results_df.to_csv(results_csv, index=False)
            
            # å‰µå»ºçµ±ä¸€çš„æŠ˜ç·šåœ–
            plot_predictor_comparison(all_stats, imgName, method, comparison_dir)
            
            # è¨˜éŒ„åˆ°æ—¥èªŒ
            with open(log_file, 'a') as f:
                f.write(f"\nComparison completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"Total processing time: {time.time() - start_time:.2f} seconds\n")
                f.write("\nSummary Results:\n")
                f.write(results_df.to_string())
            
            print(f"\nComparison completed and saved to {comparison_dir}")
            print(f"Total processing time: {time.time() - start_time:.2f} seconds")
            print("\nSummary Results:")
            print(results_df)
            
            return results_df, all_stats
            
        except Exception as e:
            print(f"Error generating comparison: {str(e)}")
            with open(log_file, 'a') as f:
                f.write(f"\nError generating comparison: {str(e)}\n")
                f.write(traceback.format_exc())
    
    else:
        print("No valid statistics available for comparison")
        with open(log_file, 'a') as f:
            f.write("\nNo valid statistics available for comparison\n")
    
    return None, None

def run_simplified_precise_measurements(origImg, imgName, method, prediction_method, ratio_of_ones, 
                                      total_embeddings=5, el_mode=0, segments=15, use_different_weights=False,
                                      split_size=2, block_base=False, quad_tree_params=None):
    """
    é‹è¡Œç²¾ç¢ºçš„æ•¸æ“šé»æ¸¬é‡ï¼Œä½†åƒ…å„²å­˜æ•¸æ“šè€Œä¸ç”¢ç”Ÿåœ–åƒå’Œåœ–è¡¨
    é©ç”¨æ–¼é proposed é æ¸¬å™¨
    
    Parameters:
    -----------
    origImg : numpy.ndarray
        åŸå§‹åœ–åƒ
    imgName : str
        åœ–åƒåç¨± (ç”¨æ–¼ä¿å­˜çµæœ)
    method : str
        ä½¿ç”¨çš„æ–¹æ³•
    prediction_method : PredictionMethod
        é æ¸¬æ–¹æ³•
    ratio_of_ones : float
        åµŒå…¥æ•¸æ“šä¸­1çš„æ¯”ä¾‹
    total_embeddings : int
        ç¸½åµŒå…¥æ¬¡æ•¸
    el_mode : int
        ELæ¨¡å¼
    segments : int
        è¦æ¸¬é‡çš„æ•¸æ“šé»æ•¸é‡
    use_different_weights : bool
        æ˜¯å¦ä½¿ç”¨ä¸åŒæ¬Šé‡
    split_size : int
        åˆ†å‰²å¤§å°
    block_base : bool
        æ˜¯å¦ä½¿ç”¨block baseæ–¹å¼
    quad_tree_params : dict
        å››å‰æ¨¹åƒæ•¸
        
    Returns:
    --------
    pandas.DataFrame
        åŒ…å«æ‰€æœ‰æ¸¬é‡çµæœçš„DataFrame
    """
    # ç¸½é‹è¡Œé–‹å§‹æ™‚é–“
    total_start_time = time.time()
    method_name = prediction_method.value
    
    # å‰µå»ºçµæœç›®éŒ„ (åƒ…ç”¨æ–¼å„²å­˜CSVï¼Œä¸å„²å­˜åœ–åƒå’Œåœ–è¡¨)
    result_dir = f"./Prediction_Error_Embedding/outcome/plots/{imgName}/data_{method_name.lower()}"
    os.makedirs(result_dir, exist_ok=True)
    
    # è¨˜éŒ„é‹è¡Œè¨­ç½®
    log_file = f"{result_dir}/simplified_measurements_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    with open(log_file, 'w') as f:
        f.write(f"Simplified measurement run started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Image: {imgName}\n")
        f.write(f"Method: {method}\n")
        f.write(f"Prediction method: {method_name}\n")
        f.write(f"Ratio of ones: {ratio_of_ones}\n")
        f.write(f"Total embeddings: {total_embeddings}\n")
        f.write(f"EL mode: {el_mode}\n")
        f.write(f"Segments: {segments}\n")
        f.write(f"Use different weights: {use_different_weights}\n")
        f.write("\n" + "="*80 + "\n\n")
    
    # æ­¥é©Ÿ1: æ‰¾å‡ºæœ€å¤§åµŒå…¥å®¹é‡
    print(f"\n{'='*80}")
    print(f"Step 1: Finding maximum payload capacity for {method_name}")
    print(f"{'='*80}")
    
    with open(log_file, 'a') as f:
        f.write(f"Step 1: Finding maximum payload capacity\n")
    
    start_time = time.time()
    final_img_max, max_payload, stages_max = run_embedding_with_target(
        origImg, method, prediction_method, ratio_of_ones, 
        total_embeddings, el_mode, target_payload_size=-1,
        split_size=split_size, block_base=block_base, 
        quad_tree_params=quad_tree_params,
        use_different_weights=use_different_weights
    )
    
    max_run_time = time.time() - start_time
    total_pixels = origImg.size
    
    print(f"Maximum payload: {max_payload} bits")
    print(f"Max BPP: {max_payload/total_pixels:.6f}")
    print(f"Time taken: {max_run_time:.2f} seconds")
    
    with open(log_file, 'a') as f:
        f.write(f"Maximum payload: {max_payload} bits\n")
        f.write(f"Max BPP: {max_payload/total_pixels:.6f}\n")
        f.write(f"Time taken: {max_run_time:.2f} seconds\n\n")
    
    # æ¸…ç†è¨˜æ†¶é«”
    cleanup_memory()
    
    # æ­¥é©Ÿ2: è¨ˆç®—å‡å‹»åˆ†å¸ƒçš„payloadé»
    print(f"\n{'='*80}")
    print(f"Step 2: Calculating {segments} evenly distributed payload points")
    print(f"{'='*80}")
    
    with open(log_file, 'a') as f:
        f.write(f"Step 2: Calculating {segments} evenly distributed payload points\n")
    
    # è¨ˆç®—æ¯å€‹ç´šè·çš„ç›®æ¨™åµŒå…¥é‡ (å¾10%åˆ°100%)
    payload_points = [int(max_payload * (i+1) / segments) for i in range(segments)]
    
    print("Target payload points:")
    for i, target in enumerate(payload_points):
        print(f"  Point {i+1}: {target} bits ({target/max_payload*100:.1f}% of max)")
    
    with open(log_file, 'a') as f:
        f.write("Target payload points:\n")
        for i, target in enumerate(payload_points):
            f.write(f"  Point {i+1}: {target} bits ({target/max_payload*100:.1f}% of max)\n")
        f.write("\n")
    
    # æ­¥é©Ÿ3: ç‚ºæ¯å€‹ç›®æ¨™é»é‹è¡ŒåµŒå…¥ç®—æ³•
    print(f"\n{'='*80}")
    print(f"Step 3: Running embedding algorithm for each target point")
    print(f"{'='*80}")
    
    with open(log_file, 'a') as f:
        f.write(f"Step 3: Running embedding algorithm for each target point\n")
    
    # è¨˜éŒ„çµæœ
    results = []
    
    # æ·»åŠ æœ€å¤§åµŒå…¥å®¹é‡çš„çµæœ
    psnr_max = calculate_psnr(origImg, final_img_max)
    ssim_max = calculate_ssim(origImg, final_img_max)
    hist_corr_max = histogram_correlation(
        np.histogram(origImg, bins=256, range=(0, 255))[0],
        np.histogram(final_img_max, bins=256, range=(0, 255))[0]
    )
    
    # å°‡100%çµæœåŠ å…¥åˆ—è¡¨
    results.append({
        'Target_Percentage': 100.0,
        'Target_Payload': max_payload,
        'Actual_Payload': max_payload,
        'BPP': max_payload / total_pixels,
        'PSNR': psnr_max,
        'SSIM': ssim_max,
        'Hist_Corr': hist_corr_max,
        'Processing_Time': max_run_time
    })
    
    # æ³¨æ„ï¼šæˆ‘å€‘ä¸ä¿å­˜åœ–åƒï¼Œåªè¨˜éŒ„æ•¸æ“š
    
    with open(log_file, 'a') as f:
        f.write(f"100.0% target (Max capacity):\n")
        f.write(f"  Target: {max_payload} bits\n")
        f.write(f"  Actual: {max_payload} bits\n")
        f.write(f"  BPP: {max_payload/total_pixels:.6f}\n")
        f.write(f"  PSNR: {psnr_max:.2f}\n")
        f.write(f"  SSIM: {ssim_max:.4f}\n")
        f.write(f"  Hist_Corr: {hist_corr_max:.4f}\n")
        f.write(f"  Time: {max_run_time:.2f} seconds\n\n")
    
    # æ¸…ç†è¨˜æ†¶é«”
    cleanup_memory()
    
    # é‹è¡Œå…¶é¤˜ç´šè·çš„æ¸¬é‡ (1åˆ°segments-1ï¼Œè·³éæœ€å¾Œä¸€å€‹å› ç‚ºå·²ç¶“æœ‰äº†maxçµæœ)
    for i, target in enumerate(tqdm(payload_points[:-1], desc=f"è™•ç† {method_name} æ•¸æ“šé»")):
        percentage = (i+1) / segments * 100
        
        print(f"\nRunning point {i+1}/{segments}: {target} bits ({percentage:.1f}% of max)")
        
        with open(log_file, 'a') as f:
            f.write(f"{percentage:.1f}% target:\n")
            f.write(f"  Target: {target} bits\n")
        
        start_time = time.time()
        final_img, actual_payload, stages = run_embedding_with_target(
            origImg, method, prediction_method, ratio_of_ones, 
            total_embeddings, el_mode, target_payload_size=target,
            split_size=split_size, block_base=block_base, 
            quad_tree_params=quad_tree_params,
            use_different_weights=use_different_weights
        )
        
        run_time = time.time() - start_time
        
        # è¨ˆç®—è³ªé‡æŒ‡æ¨™
        psnr = calculate_psnr(origImg, final_img)
        ssim = calculate_ssim(origImg, final_img)
        hist_corr = histogram_correlation(
            np.histogram(origImg, bins=256, range=(0, 255))[0],
            np.histogram(final_img, bins=256, range=(0, 255))[0]
        )
        
        # è¨˜éŒ„çµæœ
        results.append({
            'Target_Percentage': percentage,
            'Target_Payload': target,
            'Actual_Payload': actual_payload,
            'BPP': actual_payload / total_pixels,
            'PSNR': psnr,
            'SSIM': ssim,
            'Hist_Corr': hist_corr,
            'Processing_Time': run_time
        })
        
        # æ³¨æ„ï¼šæˆ‘å€‘ä¸ä¿å­˜åœ–åƒï¼Œåªè¨˜éŒ„æ•¸æ“š
        
        print(f"  Actual: {actual_payload} bits")
        print(f"  BPP: {actual_payload/total_pixels:.6f}")
        print(f"  PSNR: {psnr:.2f}")
        print(f"  SSIM: {ssim:.4f}")
        print(f"  Time: {run_time:.2f} seconds")
        
        with open(log_file, 'a') as f:
            f.write(f"  Actual: {actual_payload} bits\n")
            f.write(f"  BPP: {actual_payload/total_pixels:.6f}\n")
            f.write(f"  PSNR: {psnr:.2f}\n")
            f.write(f"  SSIM: {ssim:.4f}\n")
            f.write(f"  Hist_Corr: {hist_corr:.4f}\n")
            f.write(f"  Time: {run_time:.2f} seconds\n\n")
        
        # æ¸…ç†è¨˜æ†¶é«”
        cleanup_memory()
    
    # æŒ‰ç…§æ­£ç¢ºé †åºæ’åºçµæœ
    results.sort(key=lambda x: x['Target_Percentage'])
    
    # è½‰æ›ç‚ºDataFrame
    df = pd.DataFrame(results)
    
    # æ­¥é©Ÿ4: æ•´ç†çµæœ
    print(f"\n{'='*80}")
    print(f"Step 4: Results summary for {method_name}")
    print(f"{'='*80}")
    
    total_time = time.time() - total_start_time
    
    print(f"Total processing time: {total_time:.2f} seconds")
    print(f"Average time per point: {total_time/segments:.2f} seconds")
    print(f"Results saved to {result_dir}")
    
    with open(log_file, 'a') as f:
        f.write(f"Results summary:\n")
        f.write(f"Total processing time: {total_time:.2f} seconds\n")
        f.write(f"Average time per point: {total_time/segments:.2f} seconds\n")
        f.write(f"Results saved to {result_dir}\n\n")
        
        f.write("Data table:\n")
        f.write(df.to_string(index=False))
    
    # ä¿å­˜çµæœ
    csv_path = f"{result_dir}/simplified_measurements.csv"
    df.to_csv(csv_path, index=False)
    print(f"Results saved to {csv_path}")
    
    # æ³¨æ„ï¼šæˆ‘å€‘ä¸ç”Ÿæˆåœ–è¡¨ï¼Œåªä¿å­˜æ•¸æ“š
    
    return df

def create_wide_format_tables(all_results, output_dir):
    """
    å‰µå»ºå¯¬æ ¼å¼è¡¨æ ¼ï¼Œä¾¿æ–¼è«–æ–‡ä½¿ç”¨ï¼Œä¸¦ä¿®å¾©å¯èƒ½çš„è­¦å‘Š
    
    Parameters:
    -----------
    all_results : dict
        åŒ…å«å„é æ¸¬å™¨æ¸¬é‡çµæœçš„å­—å…¸
    output_dir : str
        è¼¸å‡ºç›®éŒ„
    """
    # å‰µå»ºPSNRè¡¨æ ¼ (åˆ—ï¼šç™¾åˆ†æ¯”ï¼Œåˆ—ï¼šé æ¸¬å™¨)
    psnr_table = {'Percentage': []}
    ssim_table = {'Percentage': []}
    hist_corr_table = {'Percentage': []}
    
    # ç¢ºå®šæ‰€æœ‰ç™¾åˆ†æ¯”å€¼
    percentages = []
    for df in all_results.values():
        percentages.extend(df['Target_Percentage'].tolist())
    
    # å»é‡ä¸¦æ’åº
    percentages = sorted(list(set(percentages)))
    psnr_table['Percentage'] = percentages
    ssim_table['Percentage'] = percentages
    hist_corr_table['Percentage'] = percentages
    
    # å¡«å……å„é æ¸¬å™¨çš„æ•¸æ“š
    for predictor, df in all_results.items():
        psnr_values = []
        ssim_values = []
        hist_corr_values = []
        
        for percentage in percentages:
            # æ‰¾åˆ°æœ€æ¥è¿‘çš„ç™¾åˆ†æ¯”è¡Œ
            closest_idx = (df['Target_Percentage'] - percentage).abs().idxmin()
            # ä½¿ç”¨ .loc[idx, 'column'] è€Œä¸æ˜¯ .loc[idx]['column'] ä¾†é¿å…è­¦å‘Š
            psnr_values.append(df.loc[closest_idx, 'PSNR'])
            ssim_values.append(df.loc[closest_idx, 'SSIM'])
            hist_corr_values.append(df.loc[closest_idx, 'Hist_Corr'])
        
        psnr_table[predictor] = psnr_values
        ssim_table[predictor] = ssim_values
        hist_corr_table[predictor] = hist_corr_values
    
    # å‰µå»ºDataFrame
    psnr_df = pd.DataFrame(psnr_table)
    ssim_df = pd.DataFrame(ssim_table)
    hist_corr_df = pd.DataFrame(hist_corr_table)
    
    # ä¿å­˜è¡¨æ ¼
    psnr_df.to_csv(f"{output_dir}/wide_format_psnr.csv", index=False)
    ssim_df.to_csv(f"{output_dir}/wide_format_ssim.csv", index=False)
    hist_corr_df.to_csv(f"{output_dir}/wide_format_hist_corr.csv", index=False)
    
    # å‰µå»ºLaTeXæ ¼å¼è¡¨æ ¼
    with open(f"{output_dir}/latex_table_psnr.txt", 'w') as f:
        f.write(psnr_df.to_latex(index=False, float_format="%.2f"))
    
    with open(f"{output_dir}/latex_table_ssim.txt", 'w') as f:
        f.write(ssim_df.to_latex(index=False, float_format="%.4f"))
    
    with open(f"{output_dir}/latex_table_hist_corr.txt", 'w') as f:
        f.write(hist_corr_df.to_latex(index=False, float_format="%.4f"))
    
    print(f"Wide format tables saved to {output_dir}")